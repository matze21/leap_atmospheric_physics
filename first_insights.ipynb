{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "import dask\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! testout everything here and let it run in the cloud !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dd.read_csv('first_10partitions.csv')\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "numSamples = data.shape[0].compute()\n",
    "idx = int(numSamples*0.7)\n",
    "train = data.loc[0:idx]\n",
    "test = data.loc[idx:numSamples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# browsing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train.head(500)\n",
    "batch.isna().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat60 = ['state_t', 'state_q0001','state_q0002','state_q0003','state_u','state_v','pbuf_ozone','pbuf_CH4','pbuf_N2O']\n",
    "feat1 = ['state_ps','pbuf_SOLIN','pbuf_LHFLX','pbuf_SHFLX','pbuf_TAUX','pbuf_TAUY','pbuf_COSZRS','cam_in_ALDIF','cam_in_ALDIR','cam_in_ASDIF','cam_in_ASDIR','cam_in_LWUP','cam_in_ICEFRAC','cam_in_LANDFRAC','cam_in_OCNFRAC','cam_in_SNOWHLAND']\n",
    "\n",
    "target60 = ['ptend_t','ptend_q0001','ptend_q0002','ptend_q0003','ptend_u','ptend_v']\n",
    "target1 = ['cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC','cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD']\n",
    "\n",
    "features60 = [] \n",
    "for f in feat60:\n",
    "    features60 = features60 + [f+'_'+str(i) for i in range(60)]\n",
    "allF = features60 + feat1\n",
    "\n",
    "targets60 = [] \n",
    "for f in target60:\n",
    "    targets60 = targets60 + [f+'_'+str(i) for i in range(60)]\n",
    "allT = targets60 + target1\n",
    "\n",
    "len(allF) + len(allT), batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetsToDrop12 = [ 'ptend_q0001', 'ptend_q0002', 'ptend_q0003', 'ptend_u', 'ptend_v']\n",
    "dropT = ['ptend_q0002_12','ptend_q0002_13','ptend_q0002_14','ptend_q0002_15'] # attention, I think i also need to predict _15\n",
    "for f in targetsToDrop12:\n",
    "    dropT = dropT + [f+'_'+str(i) for i in range(12)]\n",
    "\n",
    "allT2 = [i for i in allT if i not in dropT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = train[allF+allT].mean().compute()\n",
    "std_values = train[(allF+allT)].std().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFZ = []\n",
    "for col in allF:\n",
    "    newF = col+'_zScore'\n",
    "    train[newF] = (train[col] - mean_values[col]) / std_values[col]\n",
    "    test[newF] = (test[col] - mean_values[col]) / std_values[col]\n",
    "    allFZ.append(newF)\n",
    "\n",
    "allTZ = []\n",
    "for col in allT:\n",
    "    newF = col+'_zScore'\n",
    "    train[newF] = (train[col] - mean_values[col]) / std_values[col]\n",
    "    test[newF] = (test[col] - mean_values[col]) / std_values[col]\n",
    "    allTZ.append(newF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do it batchwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXy(partId, train, feat, tf):\n",
    "    X = train[feat].get_partition(partId).compute()\n",
    "    y = train[tf].get_partition(partId).compute()\n",
    "    # normalize\n",
    "    for f in feat:\n",
    "        X[f] = (X[f] - mean_values[f]) / std_values[f]\n",
    "    for f in tf:\n",
    "        y[f] = (y[f] - mean_values[f]) / std_values[f]\n",
    "\n",
    "    return lgb.Dataset(X, label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMRegressor(objective='regression', metric='l2', verbose=-1)\n",
    "multi_output_model = MultiOutputRegressor(lgb_model)\n",
    "\n",
    "train_data = getXy(0, train, allF, allT2)\n",
    "initial_model = multi_output_model.fit(train_data.data, train_data.label)\n",
    "\n",
    "for i in range(1,train.npartitions):\n",
    "    train_data = getXy(i, train, allF, allT2)\n",
    "    updated_model = multi_output_model.fit(lgb_model.get_params(), train_data, num_boost_round=100, init_model=initial_model.estimators_[0].booster_)\n",
    "    initial_model.estimators_[0].booster_ = booster\n",
    "# Get final model\n",
    "final_model = initial_model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in train_data.label.columns:\n",
    "    if train_data.label[f].isna().any():\n",
    "        print(f, mean_values[f], std_values[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in target_cols:\n",
    "    print(f\"Training model for target: {target}\")\n",
    "    \n",
    "    # Train initial model on first batch\n",
    "    train_data = lgb.Dataset(batches[0][0], label=batches[0][1][target])\n",
    "    params = {'objective': 'regression', 'metric': 'l2'}\n",
    "    initial_model = lgb.train(params, train_data, num_boost_round=100)\n",
    "    initial_model.save_model(f'model_{target}.txt')\n",
    "    \n",
    "    # Train on remaining batches\n",
    "    for i in range(1, len(batches)):\n",
    "        train_data = lgb.Dataset(batches[i][0], label=batches[i][1][target])\n",
    "        updated_model = lgb.train(params, train_data, num_boost_round=100, \n",
    "                                  init_model=f'model_{target}.txt', \n",
    "                                  keep_training_booster=True)\n",
    "        updated_model.save_model(f'model_{target}.txt')\n",
    "    \n",
    "    # Store the final model\n",
    "    models[target] = updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storeSales",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
