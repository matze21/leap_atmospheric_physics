{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "import dask_ml\n",
    "import dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Sending large graph.*\")\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.multiprocessing\n",
    "\n",
    "cluster = LocalCluster(processes=True,n_workers=6, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "import sys\n",
    "import pickle \n",
    "\n",
    "from data_helpers import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    'train0_25',\n",
    "    'train25_50',\n",
    "    'train50_75',\n",
    "    'train75_100'\n",
    "]\n",
    "\n",
    "# Read Parquet files from each folder into Dask DataFrames\n",
    "dfs = [dd.read_parquet(folder) for folder in folders]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "data = dd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "orig_partitions = [i for i in range(0,int(data.npartitions))]\n",
    "np.random.shuffle(orig_partitions) #shuffles inplace\n",
    "\n",
    "trainSep = int(0.7* data.npartitions)\n",
    "valEnd = data.npartitions #int(0.05* data.npartitions) + trainSep\n",
    "\n",
    "sampledPartIdxTrain = orig_partitions[0:trainSep]\n",
    "sampledPartIdxTest  = orig_partitions[trainSep:valEnd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meanDict_allT.pkl', 'rb') as f:\n",
    "    meanDict = pickle.load(f)\n",
    "\n",
    "with open('stdDict_allT.pkl', 'rb') as f:\n",
    "    stdDict = pickle.load(f)\n",
    "\n",
    "with open('minVal_allT2.pkl', 'rb') as f:\n",
    "    minDict = pickle.load(f)\n",
    "\n",
    "with open('zScore_allT2.pkl', 'rb') as f:\n",
    "    zscoreDict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min0 = 1000\n",
    "minT = ''\n",
    "max0 = 0\n",
    "maxT = ''\n",
    "for f in allT2:\n",
    "    #if f == 'cam_out_SOLS' or f == 'cam_out_SOLL' or f == 'cam_out_NETSW':\n",
    "    #    continue\n",
    "    if minDict[f]['min'] < min0:\n",
    "        min0=minDict[f]['min']\n",
    "        minT = f\n",
    "    if minDict[f]['max'] > max0:\n",
    "        max0=minDict[f]['max']\n",
    "        maxT = f\n",
    "\n",
    "# single features\n",
    "# sols and soll features are crazy small, to 1e-300 (log -> -744, with lots of values), next one is ptend_q0002_21 to 2e-62 (log -> 528)\n",
    "# max can be up to 1100 for netsw (log -> 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min0, minT, max0, maxT, np.log(min0/max0), np.log(max0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseTrain=[]\n",
    "nPartitions=20\n",
    "for i in range(int(nPartitions/5)):\n",
    "    baseTrain.append(data.partitions[sampledPartIdxTrain[i*5:i*5+5]].compute())\n",
    "baseTrain = pd.concat(baseTrain)\n",
    "\n",
    "baseVal=[]\n",
    "nPartitions=10\n",
    "for i in range(int(nPartitions/5)):\n",
    "    baseVal.append(data.partitions[sampledPartIdxTest[i*5:i*5+5]].compute())\n",
    "baseVal = pd.concat(baseVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largeV = pd.read_parquet('large_training_df_0001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = int(largeV.shape[0]*1)\n",
    "end = largeV.shape[0]-1\n",
    "train=pd.concat([baseTrain, largeV.iloc[0:sep]])\n",
    "val = pd.concat([baseVal, largeV.iloc[sep:end]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del baseTrain, baseVal, largeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the max value of each feature is in the set\n",
    "for f in allT2:\n",
    "    if max(abs(train[f])) < minDict[f]['max']*0.95:\n",
    "        print(f, max(abs(train[f])), minDict[f]['max'])\n",
    "\n",
    "# TODO: try with a better dataset -> find all max values of all targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transform targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "absolute\n",
    "\"\"\"\n",
    "def custom_log_3(x, minValue, maxValue):  #offset of works for [-403:403] of x values otherwise sign is lost\n",
    "    y = abs(x.copy())\n",
    "    minValue = max(1e-60, abs(minValue)) #map all smaller things to 0\n",
    "    \n",
    "    nullValueFeat = abs(minValue)\n",
    "    y[y<minValue] = nullValueFeat                             # will make problems bc 0 could be positive but also negative! dynamics will point in different directions\n",
    "    \n",
    "    y = y/maxValue  * 0.01\n",
    "    y = np.log(abs(y))\n",
    "    #y = y + abs(np.log(nullValueFeat))         #move curve down such that we have a bigger domain that always has negative values as an outcome [-403:403]\n",
    "    return y\n",
    "\n",
    "\"\"\"\n",
    "absolute\n",
    "\"\"\"\n",
    "def inv_custom_log_3(y,minValue, maxValue):\n",
    "    #minValue = max(1e-60, abs(minValue))\n",
    "    nullValueFeat = abs(minValue)\n",
    "    nullValueLog  = abs(np.log(abs(nullValueFeat)))\n",
    "    #print(nullValueFeat, nullValueLog)\n",
    "\n",
    "    x = y.copy()\n",
    "    #x = np.clip(x, 0, 1e300) #can not have something smaller than 0\n",
    "    #x = x - nullValueLog                                                        # add offset\n",
    "    x = np.exp(x)\n",
    "    x = np.clip(x,1e-300,0.01)\n",
    "    x = x*maxValue/0.01\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "continuous\n",
    "\"\"\"\n",
    "def custom_log_4(x, minValue, maxValue):  #offset of works for [-403:403] of x values otherwise sign is lost\n",
    "    y = abs(x.copy())\n",
    "    minValue = max(1e-30, abs(minValue)) #map all smaller things to 0\n",
    "    \n",
    "    nullValueFeat = abs(minValue)\n",
    "    y[y<minValue] = nullValueFeat                             # will make problems bc 0 could be positive but also negative! dynamics will point in different directions\n",
    "    \n",
    "    y = y/maxValue  * 0.0001  # all values are max 0.01\n",
    "    y = np.log(abs(y))      # -> all values are negative\n",
    "\n",
    "    offset = -np.log(nullValueFeat/maxValue *0.0001)\n",
    "    y = y + offset # -> all values are positive\n",
    "    y = abs(y) * np.sign(x)\n",
    "    #y = y + abs(np.log(nullValueFeat))         #move curve down such that we have a bigger domain that always has negative values as an outcome [-403:403]\n",
    "    return y\n",
    "\n",
    "\"\"\"\n",
    "continuous region\n",
    "\"\"\"\n",
    "def inv_custom_log_4(y,minValue, maxValue):\n",
    "    minValue = max(1e-30, abs(minValue))\n",
    "    nullValueFeat = abs(minValue)\n",
    "    nullValueLog  = abs(np.log(abs(nullValueFeat)))\n",
    "    offset = -np.log(nullValueFeat/maxValue *0.0001)\n",
    "\n",
    "    x = y.copy()\n",
    "    x = abs(x)\n",
    "    x = x - offset\n",
    "    #x = np.clip(x, 0, 1e300) #can not have something smaller than 0\n",
    "    #x = x - nullValueLog                                                        # add offset\n",
    "    x = np.exp(x)\n",
    "    x = np.clip(x, 0, 1e300)\n",
    "    x = x * np.sign(y)\n",
    "    x = x*maxValue/0.0001\n",
    "    return x\n",
    "\n",
    "def custom_log_5(x, minValue, maxValue):  #offset of works for [-403:403] of x values otherwise sign is lost\n",
    "    y = abs(x.copy())\n",
    "    minValue = max(1e-30, abs(minValue)) #map all smaller things to 0\n",
    "    y[y<minValue] = minValue\n",
    "\n",
    "    y = 1/y                 # take inverse, small values get very large\n",
    "    #print(y)\n",
    "    y = y / (1/minValue)    # normalize to 1, initial large values get very small\n",
    "    print(y)\n",
    "    y = np.log(y)           # -> all values are negative\n",
    "    print(y)\n",
    "    y = abs(y) * np.sign(x)\n",
    "    return y\n",
    "\n",
    "\"\"\"\n",
    "continuous region\n",
    "\"\"\"\n",
    "def inv_custom_log_5(y,minValue, maxValue):\n",
    "    minValue = max(1e-30, abs(minValue))\n",
    "\n",
    "    x = -abs(y.copy())\n",
    "    x = np.exp(x)\n",
    "    x = x * (1/minValue)\n",
    "    x = 1/x\n",
    "    x = x * np.sign(y)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'ptend_q0002_55'\n",
    "logF = f+'_log'\n",
    "invLogF = logF+'_inv'\n",
    "train[logF] = custom_log_3(train[f], min0, max0)\n",
    "train[invLogF] = inv_custom_log_3(train[logF], min0, max0)\n",
    "\n",
    "train[[f, logF, invLogF]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'ptend_q0002_55'\n",
    "logF = f+'_log'\n",
    "invLogF = logF+'_inv'\n",
    "train[logF] = custom_log_4(train[f], min0, max0)\n",
    "train[invLogF] = inv_custom_log_4(train[logF], min0, max0)\n",
    "\n",
    "train[[f, logF, invLogF]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'ptend_q0002_55'\n",
    "logF = f+'_log'\n",
    "invLogF = logF+'_inv'\n",
    "train[logF] = custom_log_5(train[f], min0, max0)\n",
    "train[invLogF] = inv_custom_log_5(train[logF], min0, max0)\n",
    "\n",
    "train[[f, logF, invLogF]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logFeatures = []\n",
    "for f in allT2:\n",
    "    logF = f+'_log'\n",
    "    #train[logF] = custom_log_3(train[f], min0)\n",
    "    #val[logF] = custom_log_3(val[f], min0)\n",
    "    train[logF] = custom_log_3(train[f], minDict[f]['min'],minDict[f]['max'])\n",
    "    val[logF] = custom_log_3(val[f], minDict[f]['min'],minDict[f]['max'])\n",
    "    logFeatures.append(logF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logFeatures = []\n",
    "for f in allT2:\n",
    "    logF = f+'_log'\n",
    "    #train[logF] = custom_log_3(train[f], min0)\n",
    "    #val[logF] = custom_log_3(val[f], min0)\n",
    "    train[logF] = custom_log_4(train[f], minDict[f]['min'],minDict[f]['max'])\n",
    "    val[logF] = custom_log_4(val[f], minDict[f]['min'],minDict[f]['max'])\n",
    "    logFeatures.append(logF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logFeatures = []\n",
    "for f in allT2:\n",
    "    logF = f+'_log'\n",
    "    #train[logF] = custom_log_3(train[f], min0)\n",
    "    #val[logF] = custom_log_3(val[f], min0)\n",
    "    train[logF] = custom_log_5(train[f], minDict[f]['min'],minDict[f]['max'])\n",
    "    val[logF] = custom_log_5(val[f], minDict[f]['min'],minDict[f]['max'])\n",
    "    logFeatures.append(logF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMat = train[allF+logFeatures].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "sns.heatmap(corrMat)\n",
    "plt.title(f\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lgbms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    #'num_leaves': 15,\n",
    "    'num_threads': 7,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "i = 0\n",
    "log = {}\n",
    "for f in ['cam_out_SOLL', 'ptend_u_21','ptend_q0002_55','ptend_q0002_26','cam_out_FLWDS']:#allT2:\n",
    "    print('processing ',f)\n",
    "    fileName = 'individualLGBMs_log/model_'+f+'.txt'\n",
    "    logF = f+'_log'\n",
    "    gbm = lgb.Booster(model_file=fileName) if i != 0 else None\n",
    "    trainSet = lgb.Dataset(train[allF], label=train[logF], free_raw_data=False)\n",
    "    valSet = lgb.Dataset(val[allF], val[logF], free_raw_data=False)\n",
    "    gbm = lgb.train(params,\n",
    "                trainSet,\n",
    "                num_boost_round=500, \n",
    "                valid_sets=valSet,\n",
    "                init_model=gbm)\n",
    "    \n",
    "    predTrain0 = gbm.predict(train[allF])\n",
    "    predVal0 = gbm.predict(val[allF])\n",
    "    #predTrain = inv_custom_log_3(predTrain0, min0)\n",
    "    #predVal = inv_custom_log_3(predVal0, min0)\n",
    "    predTrain = inv_custom_log_5(predTrain0, minDict[f]['min'],minDict[f]['max'])\n",
    "    predVal = inv_custom_log_5(predVal0, minDict[f]['min'],minDict[f]['max'])\n",
    "    r2train =r2_score(abs(train[f]), predTrain)\n",
    "    r2test =r2_score(abs(val[f]), predVal)\n",
    "    r2trainT = r2_score(train[logF], predTrain0)\n",
    "    r2testT = r2_score(val[logF], predVal0)\n",
    "    print('r2 scores', r2train,r2test, 'transormed',r2trainT,r2testT)\n",
    "    log[f]={'train':r2train,'test':r2test,'trainTransf':r2trainT,'testTransf':r2testT}\n",
    "\n",
    "    plt.scatter(x=range(train.shape[0]),y=abs(train[f]), s=1,label=f)\n",
    "    plt.scatter(x=range(train.shape[0]),y=predTrain, s=1,label='pred_train')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.scatter(x=range(train.shape[0]),y=predTrain0, s=1,label='pred_train')\n",
    "    plt.scatter(x=range(train.shape[0]),y=train[logF], s=1,label=f)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    gbm.save_model(fileName)\n",
    "    gbm.save_model('individualLGBMs_log/checkpoints/model_'+f+'_'+str(i)+'_'+str(round(r2test,3))+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" in feature space \"\"\"\n",
    "plt.scatter(x=range(train.shape[0]),y=abs(train[f]), s=1,label=f)\n",
    "plt.scatter(x=range(train.shape[0]),y=predTrain, s=1,label='pred_train')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=range(val.shape[0]),y=abs(val[f]), s=1,label=f)\n",
    "plt.scatter(x=range(val.shape[0]),y=predVal, s=1,label='pred_test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" in transformed space \"\"\"\n",
    "plt.scatter(x=range(train.shape[0]),y=predTrain0, s=1,label='pred_train')\n",
    "plt.scatter(x=range(train.shape[0]),y=train[logF], s=1,label=f)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=range(val.shape[0]),y=predVal0, s=1,label='pred_test')\n",
    "plt.scatter(x=range(val.shape[0]),y=val[logF], s=1,label=f)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Concatenate,BatchNormalization, Reshape\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "numF = len(allF)\n",
    "numT = len(allT2)\n",
    "\n",
    "input = Input(shape=(numF))\n",
    "\n",
    "x = BatchNormalization()(input)\n",
    "#x = Dense(numF, activation='relu')(input)\n",
    "\n",
    "print(x.shape)\n",
    "for i in range(1):\n",
    "    x = Dense((i+1)*numF, activation='relu')(x)\n",
    "    print(x.shape)\n",
    "for i in range(1):\n",
    "    x = Dense(1/(i+1)*x.shape[1], activation='relu')(x)\n",
    "    print(x.shape)\n",
    "output = Dense(numT, activation='linear',name='output')(x)\n",
    "print(output.shape)\n",
    "\n",
    "model = Model(inputs=input, outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse', metrics=[RSquaredMetric()])\n",
    "#model.summary()\n",
    "\n",
    "hist = model.fit(train[allF], train[logFeatures], epochs=25, batch_size=512, validation_data=(val[allF],val[logFeatures]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTrain0 = model.predict(train[allF])\n",
    "predVal0 = model.predict(val[allF])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTrain0 = pd.DataFrame(predTrain0, columns=logFeatures)\n",
    "predVal0 = pd.DataFrame(predVal0, columns=logFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in allT2:\n",
    "    logF = f+'_log'\n",
    "    invF = logF+'i'\n",
    "    predTrain0[invF] = inv_custom_log_3(predTrain0[logF], min0)\n",
    "    predVal0[invF] = inv_custom_log_3(predVal0[logF], min0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in allT2:\n",
    "    logF = f+'_log'\n",
    "    invF = logF+'i'\n",
    "    r2train =r2_score(train[f], predTrain0[invF])\n",
    "    r2test =r2_score(val[f], predVal0[invF])\n",
    "    print(f,'r2 scores', r2train,r2test, 'transormed',r2_score(train[logF], predTrain0[logF]),r2_score(val[logF], predVal0[logF]))\n",
    "    \"\"\" in feature space \"\"\"\n",
    "    #plt.scatter(x=range(train.shape[0]),y=train[f], s=1,label=f)\n",
    "    #plt.scatter(x=range(train.shape[0]),y=predTrain0[invF], s=1,label='pred_train')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    \"\"\" in transformed space \"\"\"\n",
    "    plt.scatter(x=range(train.shape[0]),y=predTrain0[logF], s=1,label='pred_train')\n",
    "    plt.scatter(x=range(train.shape[0]),y=train[logF], s=1,label=f)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTrain0.loc[predTrain0[invF] > 10000000000000000][[invF,logF]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
