{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "import dask_ml\n",
    "import dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Sending large graph.*\")\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.multiprocessing\n",
    "\n",
    "cluster = LocalCluster(processes=True,n_workers=6, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "import sys\n",
    "import pickle \n",
    "\n",
    "from data_helpers import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    'train0_25',\n",
    "    'train25_50',\n",
    "    'train50_75',\n",
    "    'train75_100'\n",
    "]\n",
    "\n",
    "# Read Parquet files from each folder into Dask DataFrames\n",
    "dfs = [dd.read_parquet(folder) for folder in folders]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "data = dd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "orig_partitions = [i for i in range(0,int(data.npartitions))]\n",
    "np.random.shuffle(orig_partitions) #shuffles inplace\n",
    "\n",
    "trainSep = int(0.7* data.npartitions)\n",
    "valEnd = data.npartitions #int(0.05* data.npartitions) + trainSep\n",
    "\n",
    "sampledPartIdxTrain = orig_partitions[0:trainSep]\n",
    "sampledPartIdxTest  = orig_partitions[trainSep:valEnd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meanDict_allT.pkl', 'rb') as f:\n",
    "    meanDict = pickle.load(f)\n",
    "\n",
    "with open('stdDict_allT.pkl', 'rb') as f:\n",
    "    stdDict = pickle.load(f)\n",
    "\n",
    "with open('minVal_allT2.pkl', 'rb') as f:\n",
    "    minDict = pickle.load(f)\n",
    "\n",
    "with open('zScore_allT2.pkl', 'rb') as f:\n",
    "    zscoreDict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseTrain = data.partitions[sampledPartIdxTrain[0:15]].compute()\n",
    "largeV_26 = pd.read_csv('large_ptend_q0002_26_allData.csv')\n",
    "largeV = pd.read_parquet('large_training_df_0001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = data.partitions[sampledPartIdxTest[15:30]].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "custom log function to map into a continuous region, gives more resolution to the small values\n",
    "\"\"\"\n",
    "def custom_log_2(x, minValue, offset=6, nullValFactor=1):  #offset of works for [-403:403] of x values otherwise sign is lost\n",
    "    nullValueFeat = -minValue*nullValFactor             # define the 0-value in the feature space\n",
    "    x[x==0] = nullValueFeat                             # will make problems bc 0 could be positive but also negative! dynamics will point in different directions\n",
    "    y = np.log(abs(x))\n",
    "    y = y - offset                                      #move curve down such that we have a bigger domain that always has negative values as an outcome [-403:403]\n",
    "    nullValueLog = np.log(abs(nullValueFeat)) - offset  # transform 0-value into log space\n",
    "    y[x>0] = nullValueLog - (y[x>0] - nullValueLog)\n",
    "    print(nullValueLog,nullValueFeat)\n",
    "    y = y - nullValueLog\n",
    "    return y\n",
    "\n",
    "\n",
    "def inv_custom_log_2(y,minDict, offset=6, nullValFactor=1):\n",
    "    nullValueFeat = -minDict['min']*nullValFactor\n",
    "    nullValueLog  = np.log(abs(nullValueFeat)) - offset \n",
    "\n",
    "    #print(nullValueLog,nullValueFeat)\n",
    "    x = y.copy()\n",
    "    x = x + nullValueLog\n",
    "    x[y<nullValueLog] = nullValueLog - (x[y<nullValueLog] - nullValueLog) # remap to log function\n",
    "    #print('mirror values',x)\n",
    "    x = x + offset                                                        # add offset\n",
    "    x = np.exp(x)                                                         # apply exp funciton (all pos values aftewards)\n",
    "    #print('exp values',x)\n",
    "    x[x<nullValueFeat] = 0                                                # map to 0\n",
    "    x[y>nullValueLog] = -x[y>nullValueLog]                                # find negative values\n",
    "\n",
    "    # clip to physical values\n",
    "    x = np.clip(x, minDict['maxNeg'], minDict['maxPos'])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[transfF] = custom_log_2(train[f].copy(), minValue=4e-23)\n",
    "val[transfF] = custom_log_2(val[f].copy(), minValue=4e-23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(train.loc[train[f] != 0][f]).sort_values(), f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = 'ptend_t_1'\n",
    "datafeat2 = data['ptend_t_1'].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(data.loc[data[f] != 0][f]).min().compute() \n",
    "# more smaller values in data that are not in training! \n",
    "# need to find a dataset with small and large values -> the extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFeat = data[f].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFeat.loc[(dataFeat !=0) & (abs(dataFeat)<1e-24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "res = ax.hist(np.log(abs(datafeat2.loc[datafeat2!=0])), bins=20)\n",
    "\n",
    "valArr = res[1]\n",
    "ax.set_xticks(valArr)\n",
    "ax.set_xticklabels(np.exp(valArr))\n",
    "\n",
    "# maybe log transform is not the best since it makes the space between 0 and the next small values so large -> rescaling might be better\n",
    "\n",
    "# for q0002_26 extreme values are sparse -> log transform works perfectly\n",
    "# for q0002_55 tiny values are sparse -> uninteresting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscoreDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find correct weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minDimNeg = abs(datafeat2.loc[datafeat2 < 0]).min()\n",
    "minDimPos = abs(datafeat2.loc[datafeat2 > 0]).min() \n",
    "maxDimNeg = abs(datafeat2.loc[datafeat2 < 0]).max()\n",
    "maxDimPos = abs(datafeat2.loc[datafeat2 > 0]).max() \n",
    "\n",
    "minDimNeg, minDimPos, maxDimNeg, maxDimPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBin(x,bins):\n",
    "        foundBin = False\n",
    "        for i in range(len(bins)-1):\n",
    "\n",
    "            if x>=bins[i] and x<=bins[i+1]:# and foundBin==False:\n",
    "                foundBin=True\n",
    "                relevantBin = bins[i]\n",
    "                break\n",
    "            # if rounding errors occure\n",
    "            if foundBin == False and i == len(bins)-2:\n",
    "                isBinCloseToTopEdge = x>=bins[i] and x<=bins[i+1]*1.1\n",
    "                isBinCloseToBottomEdge = x>=bins[0]*0.9 and x<=bins[1]\n",
    "                if isBinCloseToTopEdge:\n",
    "                    relevantBin = bins[i]\n",
    "                    foundBin=True\n",
    "                if isBinCloseToBottomEdge:\n",
    "                    relevantBin = bins[0]\n",
    "                    foundBin=True\n",
    "        if foundBin == False:\n",
    "            print('didnt find bin, something is wrong!',x)\n",
    "            return 0\n",
    "        else:\n",
    "            return relevantBin\n",
    "\n",
    "\"\"\" calc normalized weight of histogram distribution \"\"\"\n",
    "def calcNormWeightLogDist(datafeat2,binsPos=10, binsNeg=10):\n",
    "    resNeg = np.histogram(np.log(abs(datafeat2.loc[datafeat2 < 0])), bins = binsNeg)\n",
    "    resPos = np.histogram(np.log(abs(datafeat2.loc[datafeat2 > 0])), bins = binsPos)\n",
    "    nZero  = datafeat2.loc[datafeat2 == 0].shape[0]\n",
    "\n",
    "    counts, bins = resPos\n",
    "    nBinsP = len(resPos[1])\n",
    "    nBinsN = len(resNeg[1])\n",
    "    overallBins = np.zeros((nBinsP+1+nBinsN))\n",
    "    overallBins[0:nBinsN] = np.sort(-np.exp(resNeg[1]))\n",
    "    overallBins[nBinsN:nBinsN+1] = 0\n",
    "    overallBins[nBinsN+1:nBinsN+1+nBinsP] = np.exp(resPos[1])\n",
    "\n",
    "    counts, bins = np.histogram(datafeat2, bins=overallBins)\n",
    "\n",
    "    df = pd.DataFrame(datafeat2, columns=[f2])\n",
    "    df['bins'] = df[f2].apply(lambda x: findBin(x,bins))\n",
    "\n",
    "    valueCountsDict = df['bins'].value_counts()\n",
    "    df['weightBins'] = df['bins'].apply(lambda x: valueCountsDict[x])\n",
    "\n",
    "    df['normWeightBins'] = 1/df['weightBins']\n",
    "    df['normWeightBins'] = df['normWeightBins'] / df.normWeightBins.sum() # weights sum to 1\n",
    "    return df\n",
    "\n",
    "def calcNormWeightLogDistDf(df,f, binsPos=10, binsNeg=10):\n",
    "    datafeat2 = df[f]\n",
    "    resNeg = np.histogram(np.log(abs(datafeat2.loc[datafeat2 < 0])), bins = binsNeg)\n",
    "    resPos = np.histogram(np.log(abs(datafeat2.loc[datafeat2 > 0])), bins = binsPos)\n",
    "    nZero  = datafeat2.loc[datafeat2 == 0].shape[0]\n",
    "\n",
    "    counts, bins = resPos\n",
    "    nBinsP = len(resPos[1])\n",
    "    nBinsN = len(resNeg[1])\n",
    "    overallBins = np.zeros((nBinsP+1+nBinsN))\n",
    "    overallBins[0:nBinsN] = np.sort(-np.exp(resNeg[1]))\n",
    "    overallBins[nBinsN:nBinsN+1] = 0\n",
    "    overallBins[nBinsN+1:nBinsN+1+nBinsP] = np.exp(resPos[1])\n",
    "\n",
    "    counts, bins = np.histogram(datafeat2, bins=overallBins)\n",
    "\n",
    "    df['bins'] = df[f].apply(lambda x: findBin(x,bins))\n",
    "\n",
    "    valueCountsDict = df['bins'].value_counts()\n",
    "    df['weightBins'] = df['bins'].apply(lambda x: valueCountsDict[x])\n",
    "\n",
    "    df['normWeightBins'] = 1/df['weightBins']\n",
    "    df['normWeightBins'] = df['normWeightBins'] / df.normWeightBins.sum() # weights sum to 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def calcWeightZScore(df, f, meanDict, stdDict):\n",
    "    df['zScore'] = (df[f] - meanDict[f])/stdDict[f]\n",
    "    df['weight'] = abs(df['zScore'])\n",
    "    df['normWeightZScore'] = df['weight'] / df['weight'].sum() # sums weights to 1 / make them comparable\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = 'ptend_q0001_17'\n",
    "datafeat2 = data[f2].compute()\n",
    "df_q0001_17 = calcNormWeightLogDist(datafeat2)\n",
    "df_q0001_17 = calcWeightZScore(df_q0001_17, f2, meanDict, stdDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q0001_17 = df_q0001_17.drop(['bins','weightBins','zScore','weight'], axis=1)\n",
    "df_q0001_17['weight'] = df_q0001_17['normWeightBins'] + df_q0001_17['normWeightZScore']\n",
    "df_q0001_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered = largeV.loc[(largeV[f] >= (meanDict[f] + 10*stdDict[f])) | (largeV[f] <= (meanDict[f] - 10*stdDict[f]))]\n",
    "#train = pd.concat([baseTrain, filtered], axis=0)\n",
    "train = pd.concat([baseTrain, largeV], axis=0)\n",
    "\n",
    "f = 'ptend_q0001_17'\n",
    "train = calcNormWeightLogDistDf(train, f, binsNeg=50, binsPos=50)\n",
    "train = calcWeightZScore(train, f, meanDict, stdDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = []\n",
    "for bin in train.bins.unique():\n",
    "    sampled.append(train.loc[train.bins == bin].sample(n=1000, replace=True))\n",
    "sampled=pd.concat(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valSet = lgb.Dataset(val[allF], label=val[f], free_raw_data=False)\n",
    "#trainSet = lgb.Dataset(train[allF], train[f], weight=train['normWeightBins'], free_raw_data=False)\n",
    "#trainSet = lgb.Dataset(sampled[allF], sampled[f], free_raw_data=False)\n",
    "trainSet = lgb.Dataset(baseTrain[allF], baseTrain[f], free_raw_data=False)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    #'num_leaves': 15,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "\n",
    "gbm = None\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "            trainSet,\n",
    "            num_boost_round=500, \n",
    "            valid_sets=valSet,\n",
    "            init_model=gbm)\n",
    "\n",
    "predTrain = gbm.predict(train[allF])\n",
    "predVal = gbm.predict(val[allF])\n",
    "r2train =r2_score(train[f], predTrain)\n",
    "r2test =r2_score(val[f], predVal)\n",
    "print('r2 scores', r2train,r2test)# 'transormed',r2_score(train[transfF], predTrain0),r2_score(val[transfF], predVal0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" in feature space \"\"\"\n",
    "plt.scatter(x=range(train.shape[0]),y=predTrain, s=1,label='pred_train')\n",
    "plt.scatter(x=range(train.shape[0]),y=train[f], s=1,label=f)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=range(val.shape[0]),y=predVal, s=1,label='pred_test')\n",
    "plt.scatter(x=range(val.shape[0]),y=val[f], s=1,label=f)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# with both weights added: r2 scores 0.620568128775495 -12.886684419310917\n",
    "# with only bin weight:    r2 scores 0.38436556429906865 -7.106889138729176\n",
    "\n",
    "# only bin weight + filtered large Value; r2 scores 0.970014629131281 -30.927699550397456\n",
    "\n",
    "# sampled dataframe based on bin, no weight:  r2 scores 0.38266688119407166 -6.823952224204593\n",
    "# just with base train                        r2 scores 0.00028948769681691466 -0.2786449039559715 (overfitting to small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: define weighting base on:\n",
    "- how many samples are in the respective dimension: build histogram over pos & negative samples + 0 -> equally weight all of them\n",
    "- z score of samples -> the larger the more important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test weighted lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'ptend_q0001_17'\n",
    "\n",
    "filtered = largeV.loc[abs(largeV[f]) >= abs(meanDict[f])]\n",
    "filtered = largeV.loc[(largeV[f] >= (meanDict[f] + 20*stdDict[f])) | (largeV[f] <= (meanDict[f] - 20*stdDict[f]))]\n",
    "print(filtered.shape, largeV.shape)\n",
    "#train = pd.concat([baseTrain,largeV_f], axis = 0)\n",
    "#train = pd.concat([baseTrain,largeV_26], axis = 0)\n",
    "train = pd.concat([baseTrain,filtered], axis = 0)\n",
    "#train = pd.concat([baseTrain,largeV], axis = 0)\n",
    "\n",
    "valSet = lgb.Dataset(val[allF], label=val[f], free_raw_data=False)\n",
    "weight = (((train[f] - meanDict[f])/stdDict[f])**2)#specific weighting based on feature\n",
    "weight = abs(((train[f] - meanDict[f])/stdDict[f]))\n",
    "#weight = weight / min(weight)\n",
    "trainSet = lgb.Dataset(train[allF], train[f], weight=train['weight'], free_raw_data=False)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    #'num_leaves': 15,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "\n",
    "gbm = None\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "            trainSet,\n",
    "            num_boost_round=500, \n",
    "            valid_sets=valSet,\n",
    "            init_model=gbm)\n",
    "\n",
    "predTrain = gbm.predict(train[allF])\n",
    "predVal = gbm.predict(val[allF])\n",
    "r2train =r2_score(train[f], predTrain)\n",
    "r2test =r2_score(val[f], predVal)\n",
    "print('r2 scores', r2train,r2test)# 'transormed',r2_score(train[transfF], predTrain0),r2_score(val[transfF], predVal0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" in feature space \"\"\"\n",
    "plt.scatter(x=range(train.shape[0]),y=predTrain, s=1,label='pred_train')\n",
    "plt.scatter(x=range(train.shape[0]),y=train[f], s=1,label=f)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=range(val.shape[0]),y=predVal, s=1,label='pred_test')\n",
    "plt.scatter(x=range(val.shape[0]),y=val[f], s=1,label=f)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test transformed lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'ptend_q0001_17'\n",
    "minValue = minDict[f]['min']\n",
    "transfF = f+'_transf'\n",
    "\n",
    "filtered = largeV.loc[abs(largeV[f]) >= abs(meanDict[f])]\n",
    "print(filtered.shape, largeV.shape)\n",
    "#train = pd.concat([baseTrain,largeV_f], axis = 0)\n",
    "#train = pd.concat([baseTrain,largeV_26], axis = 0)\n",
    "#train = pd.concat([baseTrain,filtered], axis = 0)\n",
    "train = pd.concat([baseTrain,largeV], axis = 0)\n",
    "\n",
    "train[transfF] = custom_log_2(train[f].copy(), minValue=minValue)\n",
    "val[transfF] = custom_log_2(val[f].copy(), minValue=minValue)\n",
    "\n",
    "valSet = lgb.Dataset(val[allF], label=val[transfF], free_raw_data=False)\n",
    "weight = (((train[f] - meanDict[f])/stdDict[f])**2)#specific weighting based on feature\n",
    "weight = abs(((train[f] - meanDict[f])/stdDict[f]))\n",
    "weight = weight / min(weight)\n",
    "trainSet = lgb.Dataset(train[allF], train[transfF], weight=weight)#train['weight'], free_raw_data=False)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    #'num_leaves': 15,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "\n",
    "gbm = None\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "            trainSet,\n",
    "            num_boost_round=200, \n",
    "            valid_sets=valSet,\n",
    "            init_model=gbm)\n",
    "\n",
    "predTrain0 = gbm.predict(train[allF])\n",
    "predVal0 = gbm.predict(val[allF])\n",
    "predTrain = inv_custom_log_2(predTrain0, minDict[f])\n",
    "predVal = inv_custom_log_2(predVal0, minDict[f])\n",
    "r2train =r2_score(train[f], predTrain)\n",
    "r2test =r2_score(val[f], predVal)\n",
    "print('r2 scores', r2train,r2test, 'transormed',r2_score(train[transfF], predTrain0),r2_score(val[transfF], predVal0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTrain0 = gbm.predict(train[allF])\n",
    "predVal0 = gbm.predict(val[allF])\n",
    "predTrain = inv_custom_log_2(predTrain0, minDict[f])\n",
    "predVal = inv_custom_log_2(predVal0, minDict[f])\n",
    "r2train =r2_score(train[f], predTrain)\n",
    "r2test =r2_score(val[f], predVal)\n",
    "print('r2 scores', r2train,r2test, 'transormed',r2_score(train[transfF], predTrain0),r2_score(val[transfF], predVal0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['prediction_log'] = predTrain0\n",
    "train['prediction_feat'] = predTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" in feature space \"\"\"\n",
    "plt.scatter(x=range(train.shape[0]),y=train[f], s=1,label=f)\n",
    "plt.scatter(x=range(train.shape[0]),y=predTrain, s=1,label='pred_train')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=range(val.shape[0]),y=val[f], s=1,label=f)\n",
    "plt.scatter(x=range(val.shape[0]),y=predVal, s=1,label='pred_test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" in transformed space \"\"\"\n",
    "plt.scatter(x=range(train.shape[0]),y=predTrain0, s=1,label='pred_train')\n",
    "plt.scatter(x=range(train.shape[0]),y=train[transfF], s=1,label=f)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=range(val.shape[0]),y=predVal0, s=1,label='pred_test')\n",
    "plt.scatter(x=range(val.shape[0]),y=val[transfF], s=1,label=f)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  correlation between targets\n",
    "- only targets close to each other are strongly correlated\n",
    "- some have correlations to other variables as well\n",
    "- weak correlation of features to targets (no surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0002_f = []\n",
    "for i in range(60):\n",
    "    if i <12:\n",
    "        continue\n",
    "    q0002_f.append('ptend_q0003_'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMat = train[allF].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "sns.heatmap(corrMat)\n",
    "plt.title(f\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',None)\n",
    "corrMat['ptend_q0003_17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
