{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "import dask_ml\n",
    "import dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Sending large graph.*\")\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.multiprocessing\n",
    "\n",
    "cluster = LocalCluster(processes=True,n_workers=6, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "import sys\n",
    "import pickle \n",
    "\n",
    "from data_helpers import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=True,n_workers=6, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    'train0_25',\n",
    "    'train25_50',\n",
    "    'train50_75',\n",
    "    'train75_100'\n",
    "]\n",
    "\n",
    "# Read Parquet files from each folder into Dask DataFrames\n",
    "dfs = [dd.read_parquet(folder) for folder in folders]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "data = dd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "orig_partitions = [i for i in range(0,int(data.npartitions))]\n",
    "np.random.shuffle(orig_partitions) #shuffles inplace\n",
    "\n",
    "trainSep = int(0.7* data.npartitions)\n",
    "valEnd = data.npartitions #int(0.05* data.npartitions) + trainSep\n",
    "\n",
    "sampledPartIdxTrain = orig_partitions[0:trainSep]\n",
    "sampledPartIdxTest  = orig_partitions[trainSep:valEnd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# min Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minDict = {} #minimum value that is not 0\n",
    "for f in allT2: #['ptend_q0001_26','ptend_q0002_26']:#transfTarg60: #allT:\n",
    "    a = data[f].compute()\n",
    "    hasPos = max(a)>0\n",
    "    hasNeg = min(a)<0\n",
    "    minNeg = min(abs(a.loc[a < 0])) if hasNeg else 1e10\n",
    "    minPos = min(abs(a.loc[a > 0])) if hasPos else 1e10\n",
    "    maxPos = max(a)\n",
    "    maxNeg = abs(min(a))\n",
    "    minDict[f] = {'minNeg':minNeg, 'minPos':minPos, 'min':min(minNeg,minPos), 'maxPos':maxPos, 'maxNeg':maxNeg, 'max':max(maxPos, maxNeg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('minVal_allT2.pkl', 'wb') as f:\n",
    "    pickle.dump(minDict, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('minVal_allT2.pkl', 'rb') as f:\n",
    "    minDict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meanDict_allT.pkl', 'rb') as f:\n",
    "    meanDict = pickle.load(f)\n",
    "\n",
    "with open('stdDict_allT.pkl', 'rb') as f:\n",
    "    stdDict = pickle.load(f)\n",
    "\n",
    "with open('minVal_allT2.pkl', 'rb') as f:\n",
    "    minDict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseTrain = data.partitions[sampledPartIdxTrain[0:15]].compute()\n",
    "largeV_26 = pd.read_csv('large_ptend_q0002_26.csv')\n",
    "largeV = pd.read_parquet('large_training_df_0001')\n",
    "\n",
    "\n",
    "#with all large values: -> way worse log transform performance\n",
    "#                       -> quantile transf is stable, can't fit training well / too extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = data.partitions[sampledPartIdxTest[15:30]].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM for ptend_q0002_26 & log transf\n",
    "- looks really good with log transform + weight\n",
    "- quantile transform + weight has a hard time resolving extreme values - not as good\n",
    "\n",
    "\n",
    "looks really good if:\n",
    "- specifically filtered for large values per feature (>mean) - attention: potential loss of information due to validation leak (all large values in data used)\n",
    "- use weighting for overall features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdDict['ptend_q0002_55'], meanDict['ptend_q0002_55']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdDict['ptend_q0002_26'], meanDict['ptend_q0002_26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'ptend_q0002_55'\n",
    "sub = data.partitions[sampledPartIdxTrain]\n",
    "largeV_f = sub.loc[abs(sub[f]) > abs(meanDict[f])].compute()\n",
    "# for 55 too many samples to load into memory -> use large values from large impact file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'ptend_q0002_26'\n",
    "minValue = minDict[f]['min']\n",
    "transfF = f+'_transf'\n",
    "\n",
    "filtered = largeV.loc[abs(largeV[f]) >= abs(meanDict[f])]\n",
    "print(filtered.shape, largeV.shape)\n",
    "#train = pd.concat([baseTrain,largeV_f], axis = 0)\n",
    "#train = pd.concat([baseTrain,largeV_26], axis = 0)\n",
    "train = pd.concat([baseTrain,filtered], axis = 0)\n",
    "\n",
    "train[transfF] = custom_log(train[f].copy(), minValue=minValue)\n",
    "val[transfF] = custom_log(val[f].copy(), minValue=minValue)\n",
    "\n",
    "valSet = lgb.Dataset(val[allF], label=val[transfF], free_raw_data=False)\n",
    "weight = (((train[f] - meanDict[f])/stdDict[f])**2)#specific weighting based on feature\n",
    "#weight = 100*weight / max(weight)\n",
    "trainSet = lgb.Dataset(train[allF], train[transfF], weight=train['weight'], free_raw_data=False)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    #'num_leaves': 15,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "\n",
    "gbm = None #lgb.Booster(model_file=fileName) if i != 0 else None\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "            trainSet,\n",
    "            num_boost_round=200, \n",
    "            valid_sets=valSet,\n",
    "            init_model=gbm)\n",
    "\n",
    "predTrain0 = gbm.predict(train[allF])\n",
    "predVal0 = gbm.predict(val[allF])\n",
    "predTrain = inv_custom_log(predTrain0, minDict[f]['min'])\n",
    "predVal = inv_custom_log(predVal0, minDict[f]['min'])\n",
    "r2train =r2_score(train[f], predTrain)\n",
    "r2test =r2_score(val[f], predVal)\n",
    "print('r2 scores', r2train,r2test, 'transormed',r2_score(train[transfF], predTrain0),r2_score(val[transfF], predVal0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" in feature space \"\"\"\n",
    "plt.scatter(x=range(train.shape[0]),y=train[f], s=1,label=f)\n",
    "plt.scatter(x=range(train.shape[0]),y=predTrain, s=1,label='pred_train')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=range(val.shape[0]),y=val[f], s=1,label=f)\n",
    "plt.scatter(x=range(val.shape[0]),y=predVal, s=1,label='pred_test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" in transformed space \"\"\"\n",
    "plt.scatter(x=range(train.shape[0]),y=train[transfF], s=1,label=f)\n",
    "plt.scatter(x=range(train.shape[0]),y=predTrain0, s=1,label='pred_train')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=range(val.shape[0]),y=val[transfF], s=1,label=f)\n",
    "plt.scatter(x=range(val.shape[0]),y=predVal0, s=1,label='pred_test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTrain0 = gbm.predict(train[allF])\n",
    "predVal0 = gbm.predict(val[allF])\n",
    "predTrain = inv_custom_log(predTrain0, minDict[f]['min'])\n",
    "predVal = inv_custom_log(predVal0, minDict[f]['min'])\n",
    "r2train =r2_score(train[f], predTrain)\n",
    "r2test =r2_score(val[f], predVal)\n",
    "print('r2 scores', r2train,r2test, 'transormed',r2_score(train[transfF], predTrain0),r2_score(val[transfF], predVal0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lgbm for ptend_q0002_55\n",
    "- distribution gets all fucked up with custom log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'ptend_q0002_55'\n",
    "minValue = minDict[f]['min']\n",
    "transfF = f+'_transf'\n",
    "train[transfF] = custom_log(train[f].copy(), minValue=minValue)\n",
    "val[transfF] = custom_log(val[f].copy(), minValue=minValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=range(train.shape[0]),y=train[f], s=1,label=f)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=range(train.shape[0]),y=train[transfF], s=1,label=f)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'ptend_q0002_55'\n",
    "transfF = f+'_transf'\n",
    "qt = QuantileTransformer(n_quantiles=10000, random_state=0, output_distribution='uniform')\n",
    "train[transfF] = qt.fit_transform(train[[f]])\n",
    "val[transfF] = qt.transform(val[[f]])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(x=range(0,train.shape[0]), y=train[transfF], s=1)\n",
    "plt.scatter(x=range(0,val.shape[0]), y=val[transfF], s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fails for extreme cases\n",
    "f = 'ptend_q0002_26'\n",
    "transfF = f+'_transf'\n",
    "qt = PowerTransformer()\n",
    "train[transfF] = qt.fit_transform(train[[f]])\n",
    "val[transfF] = qt.transform(val[[f]])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(x=range(0,train.shape[0]), y=train[transfF], s=1)\n",
    "plt.scatter(x=range(0,val.shape[0]), y=val[transfF], s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt.inverse_transform(np.reshape(val[transfF],(-1,1))), val[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"------ BASE -----\"\"\"\n",
    "valSet = lgb.Dataset(val[allF], label=val[f], free_raw_data=False)\n",
    "trainSet = lgb.Dataset(train[allF], train[f])#, weight=train['weight'], free_raw_data=False)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    #'num_leaves': 15,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "\n",
    "gbm = None #lgb.Booster(model_file=fileName) if i != 0 else None\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "            trainSet,\n",
    "            num_boost_round=200, \n",
    "            valid_sets=valSet,\n",
    "            init_model=gbm)\n",
    "\n",
    "predTrain = gbm.predict(train[allF])\n",
    "predVal = gbm.predict(val[allF])\n",
    "r2train =r2_score(train[f], predTrain)\n",
    "r2test =r2_score(val[f], predVal)\n",
    "print('r2 scores', r2train,r2test, 'transormed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"------ transformed test -----\"\"\"\n",
    "valSet = lgb.Dataset(val[allF], label=val[transfF], free_raw_data=False)\n",
    "weight = ((train[f] - meanDict[f])**2) #specific weighting based on feature\n",
    "weight = 100*weight / max(weight)\n",
    "trainSet = lgb.Dataset(train[allF], label=train[transfF], weight=weight)#train['weight'], free_raw_data=False)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    #'num_leaves': 15,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "\n",
    "gbm = None #lgb.Booster(model_file=fileName) if i != 0 else None\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "            trainSet,\n",
    "            num_boost_round=200, \n",
    "            valid_sets=valSet,\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True)],\n",
    "            init_model=gbm)\n",
    "\n",
    "predTrain0 = gbm.predict(train[allF])\n",
    "predVal0 = gbm.predict(val[allF])\n",
    "predTrain = qt.inverse_transform(np.reshape(predTrain0,(-1,1)))\n",
    "predVal = qt.inverse_transform(np.reshape(predVal0,(-1,1)))\n",
    "r2train =r2_score(train[f], predTrain)\n",
    "r2test =r2_score(val[f], predVal)\n",
    "print('r2 scores', r2train,r2test, 'transormed',r2_score(train[transfF], predTrain0),r2_score(val[transfF], predVal0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" in feature space \"\"\"\n",
    "plt.scatter(x=range(train.shape[0]),y=train[f], s=1,label=f)\n",
    "plt.scatter(x=range(train.shape[0]),y=predTrain, s=1,label='pred_train')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=range(val.shape[0]),y=val[f], s=1,label=f)\n",
    "plt.scatter(x=range(val.shape[0]),y=predVal, s=1,label='pred_test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" in transformed space \"\"\"\n",
    "plt.scatter(x=range(train.shape[0]),y=train[transfF], s=1,label=f)\n",
    "plt.scatter(x=range(train.shape[0]),y=predTrain0, s=1,label='pred_train')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=range(val.shape[0]),y=val[transfF], s=1,label=f)\n",
    "plt.scatter(x=range(val.shape[0]),y=predVal0, s=1,label='pred_test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# where is the biggest error coming from in weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([baseTrain,largeV], axis = 0)\n",
    "\n",
    "for f in allT2:\n",
    "    weightF = f+'_weightContribution'\n",
    "    train[weightF] = ((train[f] - meanDict[f])/stdDict[f])**2 / train['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('Display.max_columns',10)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ptend_q0002_26_weightContribution'].sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
