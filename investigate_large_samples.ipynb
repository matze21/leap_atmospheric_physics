{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "import dask_ml\n",
    "import dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Sending large graph.*\")\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.multiprocessing\n",
    "\n",
    "cluster = LocalCluster(processes=True,n_workers=6, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "import sys\n",
    "import pickle \n",
    "\n",
    "from data_helpers import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=True,n_workers=6, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    'train0_25',\n",
    "    'train25_50',\n",
    "    'train50_75',\n",
    "    'train75_100'\n",
    "]\n",
    "\n",
    "# Read Parquet files from each folder into Dask DataFrames\n",
    "dfs = [dd.read_parquet(folder) for folder in folders]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "data = dd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptend_q002 = []\n",
    "for i in range(60):\n",
    "    ptend_q002.append('ptend_q0002_'+str(i))\n",
    "\n",
    "targetsToDrop12 = [ 'ptend_q0001', 'ptend_q0002', 'ptend_q0003', 'ptend_u', 'ptend_v']\n",
    "dropT = [] #'ptend_q0002_12','ptend_q0002_13','ptend_q0002_14'] # attention, I think i also need to predict _15\n",
    "for f in targetsToDrop12:\n",
    "    dropT = dropT + [f+'_'+str(i) for i in range(12)]\n",
    "\n",
    "allT2 = [i for i in allT if i not in dropT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean & stddev computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanDict ={}\n",
    "for f in allT:\n",
    "    meanDict[f] = data[f].mean().compute()\n",
    "\n",
    "with open('meanDict_allT.pkl', 'wb') as f:\n",
    "    pickle.dump(meanDict, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdDict ={}\n",
    "for f in allT:\n",
    "    stdDict[f] = data[f].std().compute()\n",
    "\n",
    "with open('stdDict_allT.pkl', 'wb') as f:\n",
    "    pickle.dump(stdDict, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxDict ={}\n",
    "for f in allT:\n",
    "    maxDict[f] = abs(data[f]).max().compute()\n",
    "\n",
    "with open('maxDict_allT.pkl', 'wb') as f:\n",
    "    pickle.dump(maxDict, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meanDict_allT.pkl', 'rb') as f:\n",
    "    meanDict = pickle.load(f)\n",
    "\n",
    "with open('stdDict_allT.pkl', 'rb') as f:\n",
    "    stdDict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transform df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_squared_error(row, mean_values,std_values, columns):\n",
    "    return sum(((row[col] - mean_values[col])/std_values[col]) ** 2 for col in columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_error_per_row = data.apply(lambda row: calculate_squared_error(row, meanDict, stdDict, allT2), axis=1, meta=('x', 'f8'))\n",
    "\n",
    "# Compute the result\n",
    "result = squared_error_per_row.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('sumSquaredError.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('sumSquaredError.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normRes = result['0']/max(result['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normRes = normRes.reset_index()#.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normRes.loc[normRes[0] > 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.assign(norm_weight = normRes[0].values)\n",
    "dask_series = dd.from_pandas(normRes, npartitions=data.npartitions)#,chunks=data.npartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "dask_array = da.from_array(normRes[0], chunks=data.partitions[0].shape[0].compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by assign function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(normRes), max(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.assign(weight=data.apply(lambda row: calculate_squared_error(row, meanDict, stdDict, allT2), axis=1, meta=('x', 'f8')))\n",
    "data1['normWeight'] = data1['weight'] / max(result['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largeSamp = data1.loc[(data1['normWeight'] > 0.0001)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largeSamp = largeSamp.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = dd.from_pandas(normRes, npartitions=data.npartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.assign(new_index=new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('new_index', sorted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do it per partition and write it to source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    #'testPar',\n",
    "    'train0_25',\n",
    "    'train25_50',\n",
    "    'train50_75',\n",
    "    'train75_100'\n",
    "]\n",
    "\n",
    "def calculate_squared_errorPart(partition, mean_values, std_values, columns):\n",
    "    #return sum(((row[col] - mean_values[col])/std_values[col]) ** 2 for col in columns)\n",
    "    partition['weight'] = partition.apply(lambda row : calculate_squared_error(row, mean_values, std_values, columns),axis=1)\n",
    "    #partition['weight'] = 1\n",
    "    return partition\n",
    "\n",
    "for folder in folders:\n",
    "    data = dd.read_parquet(folder)\n",
    "\n",
    "    data = data.map_partitions(\n",
    "        calculate_squared_errorPart,\n",
    "        mean_values=meanDict,\n",
    "        std_values=stdDict,\n",
    "        columns=allT2,\n",
    "        #meta=('squared_error', 'float64')\n",
    "        )\n",
    "    \n",
    "    data.to_parquet(folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxVal = data['weight'].max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    'testPar',\n",
    "    'train0_25',\n",
    "    'train25_50',\n",
    "    'train50_75',\n",
    "    'train75_100'\n",
    "]\n",
    "\n",
    "def calculate_norm_weight(partition, maxVal):\n",
    "    #return sum(((row[col] - mean_values[col])/std_values[col]) ** 2 for col in columns)\n",
    "    partition['norm_weight'] = partition['weight'] / maxVal\n",
    "    #partition['weight'] = 1\n",
    "    return partition\n",
    "\n",
    "\n",
    "for folder in folders:\n",
    "    data = dd.read_parquet(folder)\n",
    "\n",
    "    data = data.map_partitions(\n",
    "        calculate_norm_weight,\n",
    "        maxVal = maxVal\n",
    "        )\n",
    "    \n",
    "    data.to_parquet(folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get large values per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator=10\n",
    "condition = None\n",
    "subF = []\n",
    "for i in range(20,27):\n",
    "    f = 'ptend_q0002_' + str(i)\n",
    "    subF.append(f)\n",
    "    if condition is None:\n",
    "        condition = (abs(data[f]) > separator*abs(meanDict[f]))\n",
    "    else:\n",
    "        condition = condition | (abs(data[f]) > separator*abs(meanDict[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_large = data.loc[condition]\n",
    "tr_large = tr_large.compute()\n",
    "# 5.5 min for sep1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1\n",
    "trShape = tr_large.shape[0]\n",
    "for f in subF:\n",
    "    count26 = tr_large.loc[abs(tr_large[f]) > a*abs(meanDict[f])].shape[0]\n",
    "    print(f, count26, round(count26/trShape, 2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_large.loc[(abs(tr_large['ptend_q0002_26']) > separator*abs(meanDict['ptend_q0002_26'])) | (abs(tr_large['ptend_q0002_25']) > separator*abs(meanDict['ptend_q0002_25']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator=3\n",
    "tr_large_25 = data.loc[(abs(data['ptend_q0002_25']) > separator*abs(meanDict['ptend_q0002_25']))]\n",
    "tr_large_25 = tr_large_25.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator=3\n",
    "tr_large_27 = data.loc[(abs(data['ptend_q0002_27']) > separator*abs(meanDict['ptend_q0002_27']))]\n",
    "tr_large_27 = tr_large_27.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=range(tr_large.shape[0]),y=tr_large['ptend_q0002_26'], s=1,label='ptend_q0002_26')\n",
    "plt.scatter(x=range(tr_large.shape[0]),y=tr_large['ptend_q0002_25'], s=1,label='ptend_q0002_25')\n",
    "plt.scatter(x=range(tr_large.shape[0]),y=tr_large['ptend_q0002_24'], s=1,label='ptend_q0002_24')\n",
    "plt.scatter(x=range(tr_large.shape[0]),y=tr_large['ptend_q0002_27'], s=1,label='ptend_q0002_27')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze norm weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_tr = [\n",
    "    'train0_25',\n",
    "    'train25_50',\n",
    "    'train50_75',\n",
    "]\n",
    "folders_te = [\n",
    "    'train75_100'\n",
    "]\n",
    "\n",
    "dfs = [dd.read_parquet(folder) for folder in folders_tr]\n",
    "train = dd.concat(dfs)\n",
    "\n",
    "dfs = [dd.read_parquet(folder) for folder in folders_te]\n",
    "test = dd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_tr = train.loc[train['norm_weight'] > 0.0001].compute()\n",
    "# 0.001 ~6min, 1250 samples\n",
    "# 0.0005 ~8.5min, 4166 samples (min weight = 13126)\n",
    "# 0.0001 ~11.5min, 90485 (min weight 2625) -> too many unimportant samples I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_tr.to_parquet('large_training_df_0001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 100k small samples\n",
    "small_sample_size = train.shape[0].compute()\n",
    "small_tr = train.sample(frac=100000/small_sample_size).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tr.to_parquet('small_training_df_0001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('leap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d127f3b59cdcbede3105ef79393826088284249e767f609c5a6124df566c40a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
