{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "import dask_ml\n",
    "import dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Sending large graph.*\")\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.multiprocessing\n",
    "\n",
    "cluster = LocalCluster(processes=True,n_workers=6, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    'train0_25',\n",
    "    'train25_50',\n",
    "    'train50_75',\n",
    "    'train75_100'\n",
    "]\n",
    "\n",
    "# Read Parquet files from each folder into Dask DataFrames\n",
    "dfs = [dd.read_parquet(folder) for folder in folders]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "data = dd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat60 = ['state_t', 'state_q0001','state_q0002','state_q0003','state_u','state_v','pbuf_ozone','pbuf_CH4','pbuf_N2O']\n",
    "feat1 = ['state_ps','pbuf_SOLIN','pbuf_LHFLX','pbuf_SHFLX','pbuf_TAUX','pbuf_TAUY','pbuf_COSZRS','cam_in_ALDIF','cam_in_ALDIR','cam_in_ASDIF','cam_in_ASDIR','cam_in_LWUP','cam_in_ICEFRAC','cam_in_LANDFRAC','cam_in_OCNFRAC','cam_in_SNOWHLAND']\n",
    "\n",
    "target60 = ['ptend_t','ptend_q0001','ptend_q0002','ptend_q0003','ptend_u','ptend_v']\n",
    "target1 = ['cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC','cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD']\n",
    "\n",
    "features60 = [] \n",
    "for f in feat60:\n",
    "    features60 = features60 + [f+'_'+str(i) for i in range(60)]\n",
    "allF = features60 + feat1\n",
    "\n",
    "targets60 = [] \n",
    "for f in target60:\n",
    "    targets60 = targets60 + [f+'_'+str(i) for i in range(60)]\n",
    "allT = targets60 + target1\n",
    "\n",
    "targetsToDrop12 = [ 'ptend_q0001', 'ptend_q0002', 'ptend_q0003', 'ptend_u', 'ptend_v']\n",
    "dropT = ['ptend_q0002_12','ptend_q0002_13','ptend_q0002_14'] # attention, I think i also need to predict _15\n",
    "for f in targetsToDrop12:\n",
    "    dropT = dropT + [f+'_'+str(i) for i in range(12)]\n",
    "\n",
    "allT2 = [i for i in allT if i not in dropT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "orig_partitions = [i for i in range(0,int(data.npartitions))]\n",
    "np.random.shuffle(orig_partitions) #shuffles inplace\n",
    "\n",
    "trainSep = int(0.95* data.npartitions)\n",
    "valEnd = data.npartitions #int(0.05* data.npartitions) + trainSep\n",
    "\n",
    "sampledPartIdxTrain = orig_partitions[0:trainSep]\n",
    "sampledPartIdxTest  = orig_partitions[trainSep:valEnd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n60Feat = len(feat60)\n",
    "n1dFeat = len(feat1)\n",
    "n60Targ = len(target60)\n",
    "n1dTarg = len(target1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Concatenate,BatchNormalization, Reshape\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def r2_scoretf(y_true, y_pred):\n",
    "    sum_squares_residuals = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "    sum_squares_total = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)), axis=0)\n",
    "    r2 = 1 - (sum_squares_residuals / sum_squares_total)\n",
    "    return r2 #tf.reduce_mean(r2)\n",
    "\n",
    "def r2_scoreTrain(y_true, y_pred):\n",
    "    sum_squares_residuals = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "    sum_squares_total = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)), axis=0)\n",
    "    r2 = (sum_squares_residuals / sum_squares_total) # alwaysPositive, the smaller the better\n",
    "    return tf.reduce_mean(r2)\n",
    "\n",
    "class RSquaredMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='r_squared', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_sum_squares = None#self.add_weight(name='total_sum_squares', initializer='zeros', shape=shape)\n",
    "        self.residual_sum_squares = None#self.add_weight(name='residual_sum_squares', initializer='zeros', shape=shape)\n",
    "        self.num_samples = self.add_weight(name=\"num_samples\", initializer='zeros',dtype=tf.int32)\n",
    " \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, self._dtype)\n",
    "        y_pred = tf.cast(y_pred, self._dtype)\n",
    "        \n",
    "        sum_squares_residuals = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        sum_squares_total = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        sum_squares_total = tf.where(tf.equal(sum_squares_total, 0.0), tf.ones_like(sum_squares_total), sum_squares_total)\n",
    "        \n",
    "        if self.total_sum_squares is None:\n",
    "            self.total_sum_squares = self.add_weight(name='total_sum_squares', initializer='zeros', shape=sum_squares_total.shape)\n",
    "            self.residual_sum_squares = self.add_weight(name='residual_sum_squares', initializer='zeros', shape=sum_squares_residuals.shape)\n",
    "\n",
    "        self.total_sum_squares.assign_add(sum_squares_total)\n",
    "        self.residual_sum_squares.assign_add(sum_squares_residuals)\n",
    "\n",
    "    def result(self):\n",
    "        r_squared = 1 - (self.residual_sum_squares / self.total_sum_squares)\n",
    "        r_squared = tf.where(tf.math.is_nan(r_squared), tf.ones_like(r_squared), r_squared)\n",
    "        return tf.reduce_mean(r_squared)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_sum_squares.assign(tf.zeros_like(self.total_sum_squares))\n",
    "        self.residual_sum_squares.assign(tf.zeros_like(self.residual_sum_squares))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RSquaredMetric((60,n60Targ))\n",
    "m.update_state(y2d_val, y2d_pred)\n",
    "m.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RSquaredMetric(n1dTarg)\n",
    "m.update_state(y1d_val, y1d_val)\n",
    "m.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFeatures(a):\n",
    "    addF = []\n",
    "    colDict={}\n",
    "    for i in range(60):\n",
    "        newF = f'absWind_{i}'\n",
    "        addF.append(newF)\n",
    "        colDict[newF] =(a[f'state_u_{i}']**2 + a[f'state_v_{i}']**2)\n",
    "\n",
    "        newF = f'angleWind_{i}'\n",
    "        addF.append(newF)\n",
    "        colDict[newF] =np.arctan2(a[f'state_u_{i}']**2,a[f'state_v_{i}']**2)\n",
    "    a = pd.concat([a.reset_index(), pd.DataFrame(colDict).reset_index()], axis=1)\n",
    "\n",
    "    colDict = {}\n",
    "    for f in (feat60+['absWind']):\n",
    "        for i in range(59):\n",
    "            newF = 'diff'+f+'_'+str(i)\n",
    "            addF.append(newF)\n",
    "            colDict[newF] = (a[f+'_'+str(i+1)] - a[f+'_'+str(i)])\n",
    "    a = pd.concat([a.reset_index(), pd.DataFrame(colDict).reset_index()], axis=1)\n",
    "    return a, addF\n",
    "\n",
    "def getTensorDataSeq(data, partPerLoop, startPartIdx,sampledPartIdx):\n",
    "    X1d, X2d, y1d, y2d, X1dI, X2dI, y1dI,y2dI  = None, None, None, None, False, False, False, False\n",
    "    for j in range(partPerLoop):\n",
    "        a = data.get_partition(int(sampledPartIdx[startPartIdx+j])).compute()\n",
    "        #a, newF = addFeatures(a)\n",
    "        b = np.reshape(a[features60], (a.shape[0], n60Feat, 60))\n",
    "        b = np.transpose(b, (0,2,1))\n",
    "        X2d = np.concatenate([X2d,b], axis=0) if X2dI else b\n",
    "        b = np.reshape(a[targets60], (a.shape[0], n60Targ, 60))\n",
    "        b = np.transpose(b, (0,2,1))\n",
    "        y2d = np.concatenate([y2d,b], axis=0) if y2dI else b\n",
    "        X1d = np.concatenate([X1d,a[feat1]], axis=0) if X1dI else a[feat1]\n",
    "        y1d = np.concatenate([y1d,a[target1]], axis=0) if y1dI else a[target1]\n",
    "        X1dI, X2dI, y1dI,y2dI = True, True, True, True\n",
    "    return X1d, X2d, y1d, y2d\n",
    "\n",
    "def getTensorDataFlattend(data, partPerLoop, startPartIdx,sampledPartIdx):\n",
    "    X, y, xi, yi = None, None, False, False\n",
    "    for j in range(partPerLoop):\n",
    "        a = data.get_partition(int(sampledPartIdx[startPartIdx+j])).compute()\n",
    "        a, newF = addFeatures(a)\n",
    "        allF = features60+newF+feat1\n",
    "        b = a[allF]\n",
    "        X = np.concatenate([X,b], axis=0) if xi else b\n",
    "        \n",
    "        b = a[targets60+target1]\n",
    "        y = np.concatenate([y,b], axis=0) if yi else b\n",
    "\n",
    "        xi, yi = True, True\n",
    "    return X,y, allF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data\n",
    "partPerLoop = 35\n",
    "\n",
    "for i in range(1):\n",
    "    startPartIdx = i*partPerLoop\n",
    "    X_val, y_val, allF = getTensorDataFlattend(data, partPerLoop, startPartIdx, sampledPartIdxTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training sequentially\n",
    "partPerLoop = 50\n",
    "\n",
    "for i in range(1):\n",
    "    startPartIdx = i*partPerLoop\n",
    "    X,y, allF = getTensorDataFlattend(data, partPerLoop, startPartIdx, sampledPartIdxTrain)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTargets = targets60+target1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat60 = ['state_t', 'state_q0001','state_q0002','state_q0003','state_u','state_v','pbuf_ozone','pbuf_CH4','pbuf_N2O']\n",
    "feat1 = ['state_ps','pbuf_SOLIN','pbuf_LHFLX','pbuf_SHFLX','pbuf_TAUX','pbuf_TAUY','pbuf_COSZRS','cam_in_ALDIF','cam_in_ALDIR','cam_in_ASDIF','cam_in_ASDIR','cam_in_LWUP','cam_in_ICEFRAC','cam_in_LANDFRAC','cam_in_OCNFRAC','cam_in_SNOWHLAND']\n",
    "\n",
    "target60 = ['ptend_t','ptend_q0001','ptend_q0002','ptend_q0003','ptend_u','ptend_v']\n",
    "target1 = ['cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC','cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD']\n",
    "\n",
    "a = data.get_partition(int(sampledPartIdxTrain[0])).compute()\n",
    "\n",
    "bytes = sys.getsizeof(a)\n",
    "print(bytes/1e6)\n",
    "\n",
    "addF = []\n",
    "colDict={}\n",
    "for i in range(60):\n",
    "    newF = f'absWind_{i}'\n",
    "    addF.append(newF)\n",
    "    colDict[newF] =(a[f'state_u_{i}']**2 + a[f'state_v_{i}']**2)\n",
    "a = pd.concat([a.reset_index(), pd.DataFrame(colDict).reset_index()], axis=1)\n",
    "\n",
    "colDict = {}\n",
    "for f in (feat60+['absWind']):\n",
    "    for i in range(59):\n",
    "        newF = 'diff'+f+'_'+str(i)\n",
    "        addF.append(newF)\n",
    "        colDict[newF] = (a[f+'_'+str(i+1)] - a[f+'_'+str(i)])\n",
    "a = pd.concat([a.reset_index(), pd.DataFrame(colDict).reset_index()], axis=1)\n",
    "\n",
    "bytes = sys.getsizeof(a)\n",
    "print(bytes/1e6)\n",
    "#a = a.assign(**{f'diff'+f+'_'+str(i): (a[f+'_'+str(i+1)] - a[f+'_'+str(i)]) for i in range(59) for f in (feat60)}) #feat60+['absWind']\n",
    "#\n",
    "#a = a.assign(\n",
    "#    **{f'absWind_{i}': (a[f'state_u_{i}']**2 + a[f'state_v_{i}']**2) for i in range(60)}\n",
    "#)\n",
    "\n",
    "#a = a.assign(**{f'diff'+f+'_'+str(i): (a[f+'_'+str(i+1)] - a[f+'_'+str(i)]) for i in range(59) for f in (['absWind'])}) #feat60+['absWind']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(y, axis=0)\n",
    "std = np.std(y, axis=0)\n",
    "std[std==0] = 1\n",
    "\n",
    "yn = (y - mean) / std\n",
    "yn_val = (y_val - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffle dataset\n",
    "np.random.seed(42)\n",
    "permutation = np.random.permutation(X1d.shape[0])\n",
    "X1d = X1d[permutation]\n",
    "y1d = y1d[permutation]\n",
    "X2d = X2d[permutation]\n",
    "y2d = y2d[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple fully connected model\n",
    "- doesn't work at all so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "OneDInput = Input(shape=(n1dFeat,))\n",
    "TwoDInput = Input(shape=(60,n60Feat))\n",
    "\n",
    "x = BatchNormalization()(TwoDInput)\n",
    "x = Dense(n60Feat, activation='relu')(x)\n",
    "print(x.shape)\n",
    "for i in range(6):\n",
    "    x = Dense(n60Feat, activation='relu')(x)\n",
    "    print(x.shape)\n",
    "\n",
    "# add info to 1d output\n",
    "x0 = Dense(1, activation='relu')(x) #reduce to 1d\n",
    "x0 = x0[:,:,0]\n",
    "y = BatchNormalization()(OneDInput)\n",
    "y = Dense(n1dFeat, activation='relu')(y)\n",
    "\n",
    "y = Concatenate(axis=1)([x0, y])\n",
    "print(y.shape)\n",
    "for i in range(6):\n",
    "    y = Dense(60+n1dFeat, activation='relu')(y)\n",
    "    print(y.shape)\n",
    "y = Dense(n1dTarg, activation='linear', name='1d')(y)\n",
    "\n",
    "\n",
    "\n",
    "x = Dense(n60Targ, activation='linear',name='2d')(x)\n",
    "print(x.shape)\n",
    "output2d =x\n",
    "output1d =y\n",
    "\n",
    "model = Model(inputs=[TwoDInput, OneDInput], outputs=[output2d, output1d])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[r2_score])\n",
    "\n",
    "hist = model.fit([X2d, X1d], [y2d, y1d], epochs=10, batch_size=32, validation_data=([X2d_val, X1d_val],[y2d_val, y1d_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[y2d_pred, y1d_pred] = model.predict([X2d_val, X1d_val])\n",
    "\n",
    "y2d_pred = np.reshape(y2d_pred, (y2d_pred.shape[0],-1))\n",
    "y2d_val0 = np.reshape(y2d_val, (y2d_val.shape[0],-1))\n",
    "r2_scores = []\n",
    "f = np.reshape(np.reshape(np.array(targets60), (n60Targ,60)).transpose(), (1,-1))\n",
    "for i in range(y2d_val0.shape[1]):\n",
    "    r2 = r2_score(y2d_val0[:, i], y2d_pred[:, i])\n",
    "    print(f[0][i], r2)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "r2_scores1d = []\n",
    "for i in range(y1d_pred.shape[1]):\n",
    "    r2 = r2_score(y1d_val[:, i], y1d_pred[:, i])\n",
    "    print(target1[i], r2)\n",
    "    r2_scores1d.append(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean 2d',np.mean(np.array(r2_scores)))\n",
    "print('mean 1d',np.mean(np.array(r2_scores1d)))\n",
    "\n",
    "#oss: 2140.1865 - 2d_loss: 1.4499e-05 - 1d_loss: 2140.1865 - 2d_r2_score: -28348.7305 - 1d_r2_score: 0.9105 - val_loss: 2108.1790 - val_2d_loss: 1.6521e-06 - val_1d_loss: 2108.1790 - val_2d_r2_score: -3997.2546 - val_1d_r2_score: 0.9232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(y1d_pred, columns=target1)\n",
    "b = pd.DataFrame(y1d_val, columns=target1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.cam_out_PRECSC.plot()\n",
    "b.cam_out_PRECSC.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network for 1d and 2d\n",
    "1d:\n",
    "- snow & rain rate have big problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "OneDInput = Input(shape=(n1dFeat,))\n",
    "TwoDInput = Input(shape=(60,n60Feat), name='input')\n",
    "x = Reshape((60 * n60Feat,),name='inputReshape')(TwoDInput)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(60*n60Feat, activation='relu')(x)\n",
    "print(x.shape)\n",
    "for i in range(2):\n",
    "    x = Dense((60/(1))*n60Feat, activation='relu')(x)\n",
    "    print(x.shape)\n",
    "\n",
    "# add info to 1d output\n",
    "y = BatchNormalization()(OneDInput)\n",
    "y = Dense(n1dFeat, activation='relu')(y)\n",
    "\n",
    "commonLayer = Concatenate(axis=1)([x, y])\n",
    "commonLayerSize = 60*n60Targ+n1dFeat\n",
    "y = Dense(commonLayerSize, activation='relu')(commonLayer)\n",
    "print('y',y.shape)\n",
    "for i in range(2):\n",
    "    y = Dense(int(commonLayerSize / (2*(i+1))), activation='relu')(y)\n",
    "    print('y',y.shape)\n",
    "y = Dense(n1dTarg, activation='relu', name='1d')(y)\n",
    "print('y',y.shape)\n",
    "output1d =y\n",
    "\n",
    "x = Dense(commonLayerSize, activation='relu')(commonLayer)\n",
    "x = Dense(60*n60Targ, activation='linear')(x)\n",
    "print(x.shape)\n",
    "output2d = Reshape((60, n60Targ),name='2d')(x)\n",
    "print(output2d.shape)\n",
    "\n",
    "\n",
    "model1 = Model(inputs=[TwoDInput, OneDInput], outputs=[output1d])\n",
    "model1.compile(optimizer='adam', loss='mse', metrics=[RSquaredMetric(n1dTarg)])\n",
    "\n",
    "hist = model1.fit([X2d, X1d], y1d, epochs=10, batch_size=32, validation_data=([X2d_val, X1d_val],y1d_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1d_pred = model1.predict([X2d_val, X1d_val])\n",
    "\n",
    "r2_scores1d = []\n",
    "for i in range(y1d_pred.shape[1]):\n",
    "    r2 = r2_score(y1d_val[:, i], y1d_pred[:, i])\n",
    "    print(target1[i], r2)\n",
    "    r2_scores1d.append(r2)\n",
    "print('mean 1d',np.mean(np.array(r2_scores1d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error_sum(y_true, y_pred):\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_sum(squared_error)\n",
    "\n",
    "def r2error_mean(y_true, y_pred):\n",
    "    sum_squares_residuals = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "    sum_squares_total = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "    sum_squares_total = tf.where(tf.equal(sum_squares_total, 0.0), tf.ones_like(sum_squares_total), sum_squares_total)\n",
    "    error = (sum_squares_residuals / sum_squares_total)\n",
    "    error = tf.where(tf.math.is_nan(error), tf.ones_like(error), error)\n",
    "    return tf.reduce_mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "OneDInput = Input(shape=(n1dFeat,))\n",
    "TwoDInput = Input(shape=(60,n60Feat), name='input')\n",
    "x = Reshape((60 * n60Feat,),name='inputReshape')(TwoDInput)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(60*n60Feat, activation='relu')(x)\n",
    "print(x.shape)\n",
    "for i in range(2):\n",
    "    x = Dense((60/(1))*n60Feat, activation='relu')(x)\n",
    "    print(x.shape)\n",
    "\n",
    "# add info to 1d output\n",
    "y = BatchNormalization()(OneDInput)\n",
    "y = Dense(n1dFeat, activation='relu')(y)\n",
    "\n",
    "commonLayer = Concatenate(axis=1)([x, y])\n",
    "commonLayerSize = 60*n60Targ+n1dFeat\n",
    "y = Dense(commonLayerSize, activation='relu')(commonLayer)\n",
    "print('y',y.shape)\n",
    "for i in range(2):\n",
    "    y = Dense(int(commonLayerSize / (2*(i+1))), activation='relu')(y)\n",
    "    print('y',y.shape)\n",
    "y = Dense(n1dTarg, activation='relu', name='1d')(y)\n",
    "print('y',y.shape)\n",
    "output1d =y\n",
    "\n",
    "x = Dense(commonLayerSize, activation='relu')(commonLayer)\n",
    "for i in range(0):\n",
    "    x = Dense(commonLayerSize, activation='relu')(x)\n",
    "x = Dense(60*n60Targ, activation='linear')(x)\n",
    "print(x.shape)\n",
    "#output2d = Reshape((60, n60Targ),name='2d')(x)\n",
    "output2d = x\n",
    "print(output2d.shape)\n",
    "\n",
    "\n",
    "model2 = Model(inputs=[TwoDInput, OneDInput], outputs=output2d)\n",
    "model2.compile(optimizer='adam', loss=squared_error_sum, metrics=[RSquaredMetric((60*n60Targ))])\n",
    "\n",
    "hist = model2.fit([X2d, X1d], np.reshape(y2d, (y2d.shape[0],-1)), epochs=15, batch_size=256, validation_data=([X2d_val, X1d_val],np.reshape(y2d_val, (y2d_val.shape[0],-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2d_pred = model2.predict([X2d_val, X1d_val])\n",
    "\n",
    "y2d_pred0 = np.reshape(y2d_pred, (y2d_pred.shape[0],-1))\n",
    "y2d_val0 = np.reshape(y2d_val, (y2d_val.shape[0],-1))\n",
    "r2_scores = []\n",
    "f = np.reshape(np.reshape(np.array(targets60), (n60Targ,60)).transpose(), (1,-1))\n",
    "for i in range(y2d_val0.shape[1]):\n",
    "    r2 = r2_score(y2d_val0[:, i], y2d_pred0[:, i])\n",
    "    print(f[0][i], r2)\n",
    "    r2_scores.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(r2_scores),np.mean(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RSquaredMetric2D((60,n60Targ))\n",
    "m.update_state(y2d_val, y2d_pred)\n",
    "m.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.total_sum_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatten input fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error_sum(y_true, y_pred):\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_sum(squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "#OneDInput = Input(shape=(n1dFeat,))\n",
    "numF = len(allF)\n",
    "TwoDInput = Input(shape=(numF))\n",
    "\n",
    "#y = BatchNormalization()(OneDInput)\n",
    "#y = Dense(n1dFeat, activation='relu')(y)\n",
    "\n",
    "x = BatchNormalization()(TwoDInput)\n",
    "x = Dense(numF, activation='relu')(x)\n",
    "\n",
    "#x = Concatenate(axis=1)([x, y])\n",
    "commonSize = numF# + n1dFeat\n",
    "print(x.shape)\n",
    "for i in range(2):\n",
    "    x = Dense((i+1)*commonSize, activation='relu')(x)\n",
    "    print(x.shape)\n",
    "for i in range(2):\n",
    "    x = Dense(1/(i+1)*x.shape[1], activation='relu')(x)\n",
    "    print(x.shape)\n",
    "#for i in range(4):\n",
    "#    x = Dense(commonSize, activation='relu')(x)\n",
    "#    print(x.shape)\n",
    "#\n",
    "#y = Dense(, activation='linear',name='1d')(x)\n",
    "x = Dense(60*n60Targ+n1dTarg, activation='linear',name='2d')(x)\n",
    "\n",
    "print(x.shape)\n",
    "output2d =x\n",
    "#output1d =y\n",
    "\n",
    "#model = Model(inputs=[TwoDInput, OneDInput], outputs=[output2d])#, output1d])\n",
    "model = Model(inputs=[TwoDInput], outputs=[output2d])#, output1d])\n",
    "model.compile(optimizer='adam', loss=squared_error_sum, metrics=[RSquaredMetric()])\n",
    "model.summary()\n",
    "\n",
    "#hist = model.fit([np.reshape(X2d,(X2d.shape[0],-1)), X1d], [np.reshape(y2d,(y2d.shape[0],-1)), y1d], epochs=10, batch_size=32, validation_data=([np.reshape(X2d_val,(X2d_val.shape[0],-1)), X1d_val],[np.reshape(y2d_val,(y2d_val.shape[0],-1)), y1d_val]))\n",
    "#hist = model.fit([np.reshape(X2d,(X2d.shape[0],-1)), X1d], [np.reshape(y2dn,(y2dn.shape[0],-1))], epochs=15, batch_size=256, validation_data=([np.reshape(X2d_val,(X2d_val.shape[0],-1)), X1d_val],[np.reshape(y2dn_val,(y2dn_val.shape[0],-1))]))\n",
    "hist = model.fit(X, yn, epochs=15, batch_size=512, validation_data=(X_val,yn_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss: 46364.5664 - r_squared: 0.4725 - val_loss: 52628.4141 - val_r_squared: -2595754.2500\n",
    "y2d_pred = model.predict(X_val)\n",
    "y2d_pred = y2d_pred *std + mean\n",
    "\n",
    "r2_scores = []\n",
    "for i in range(y2d_pred.shape[1]):\n",
    "    r2 = r2_score(y_val[:, i], y2d_pred[:, i])\n",
    "    print(allTargets[i], r2)\n",
    "    r2_scores.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(r2_scores)\n",
    "#-1886738.6594913297\n",
    "#-2311049.176559818\n",
    "#-7024.681155863691  with features & normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2d_pred = model.predict(X)\n",
    "y2d_pred = y2d_pred *std + mean\n",
    "\n",
    "r2_scores = []\n",
    "for i in range(y2d_pred.shape[1]):\n",
    "    r2 = r2_score(y[:, i], y2d_pred[:, i])\n",
    "    print(allTargets[i], r2)\n",
    "    r2_scores.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size 1024\n",
    "#loss: 5.6184e-10 - r_squared: -149219557376.0000 - val_loss: 9.7947e-10 - val_r_squared: -4477976313856.0000\n",
    "\n",
    "# error = sum, bigger layer in middle helps\n",
    "#loss: 5.1245e-05 - r_squared: 0.2087 - val_loss: 5.1066e-05 - val_r_squared: 0.2107\n",
    "#bigger layer in between\n",
    "#loss: 1.7005e-05 - r_squared: -1751025319936.0000 - val_loss: 1.9207e-05 - val_r_squared: -13320339456.000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('leap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d127f3b59cdcbede3105ef79393826088284249e767f609c5a6124df566c40a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
