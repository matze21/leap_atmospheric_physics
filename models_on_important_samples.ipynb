{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "import dask_ml\n",
    "import dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Sending large graph.*\")\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.multiprocessing\n",
    "\n",
    "cluster = LocalCluster(processes=True,n_workers=6, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "import sys\n",
    "import pickle \n",
    "\n",
    "from data_helpers import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=True,n_workers=6, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Concatenate,BatchNormalization, Reshape\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_tr = pd.read_parquet('large_training_df_0001')\n",
    "small_tr = pd.read_parquet('small_training_df_0001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large_tr = large_tr.loc[large_tr.norm_weight > 0.0005]\n",
    "large_tr = large_tr.sample(n=70000, replace=True)\n",
    "small_tr = small_tr.sample(n=70000)\n",
    "\n",
    "sample_train = pd.concat([large_tr, small_tr], axis=0)\n",
    "sample_train = sample_train.sample(n=sample_train.shape[0]) #shuffle\n",
    "del large_tr, small_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_tr = [\n",
    "    'train0_25',\n",
    "    'train25_50',\n",
    "    'train50_75',\n",
    "]\n",
    "folders_te = [\n",
    "    'train75_100'\n",
    "]\n",
    "\n",
    "dfs = [dd.read_parquet(folder) for folder in folders_tr]\n",
    "train = dd.concat(dfs)\n",
    "\n",
    "dfs = [dd.read_parquet(folder) for folder in folders_te]\n",
    "test = dd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = test.shape[0].compute()\n",
    "sample_test = test.sample(frac=30000/test_size).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add features\n",
    "sample_train, addF = addFeatures(sample_train)\n",
    "sample_test, addF = addFeatures(sample_test)\n",
    "\n",
    "trainF = addF + allF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try a Fully connected network (FC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "numF = len(allF)\n",
    "numT = len(allT2)\n",
    "\n",
    "input = Input(shape=(numF))\n",
    "\n",
    "x = BatchNormalization()(input)\n",
    "#x = Dense(numF, activation='relu')(input)\n",
    "\n",
    "print(x.shape)\n",
    "for i in range(2):\n",
    "    x = Dense((i+1)*numF, activation='relu')(x)\n",
    "    print(x.shape)\n",
    "for i in range(2):\n",
    "    x = Dense(1/(i+1)*x.shape[1], activation='relu')(x)\n",
    "    print(x.shape)\n",
    "x = Dense(numT, activation='linear',name='output')(x)\n",
    "\n",
    "print(x.shape)\n",
    "output =x\n",
    "\n",
    "model = Model(inputs=input, outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[RSquaredMetric()])\n",
    "#model.summary()\n",
    "\n",
    "hist = model.fit(sample_train[allF], sample_train[allT2], epochs=15, batch_size=512, validation_data=(sample_test[allF],sample_test[allT2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(sample_test[allF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss: 8.8343 - r_squared: -25891405363985514496.0000 - val_loss: 10.8290 - val_r_squared: -39792953086422548480.0000\n",
    "# with norm_weight\n",
    "#loss: 0.0076 - r_squared: -189225829431496491027386073088.0000 - val_loss: 118.8812 - val_r_squared: -14895986782100527251456.0000\n",
    "# with weight\n",
    "#loss: 152517.7969 - r_squared: -732297370498595883287838720000.0000 - val_loss: 117.0078 - val_r_squared: -13273999613704347320320.0000\n",
    "# shuffled data with \"weight\"\n",
    "#loss: 157569.0312 - r_squared: -360110852060544499712.0000 - val_loss: 118.2304 - val_r_squared: -196862049473160990949376.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try a lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no sampling too crazy, use the 0.0001 threshold for a 50/50 sampling -> high weight on the far away samples\n",
    "r2dict = {}\n",
    "for f in ['ptend_q0002_26']:#allT2:\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'l2',\n",
    "        #'num_leaves': 15,\n",
    "        #'learning_rate': 0.05,\n",
    "        #'feature_fraction': 0.9,\n",
    "        #'bagging_fraction': 0.8,\n",
    "        #'bagging_freq': 5,\n",
    "        'verbose': -1\n",
    "    }   \n",
    "\n",
    "    gbm = lgb.train(params,\n",
    "                lgb.Dataset(sample_train[trainF], label=sample_train[f], free_raw_data=False),#, weight=sample_train['weight']),\n",
    "                num_boost_round=100, \n",
    "                valid_sets=lgb.Dataset(sample_test[trainF], label=sample_test[f], free_raw_data=False),\n",
    "                #callbacks = [lgb.early_stopping(stopping_rounds=100)],\n",
    "                init_model=None)    \n",
    "\n",
    "    predTrain = gbm.predict(sample_train[trainF])\n",
    "    predVal = gbm.predict(sample_test[trainF])\n",
    "    r2train =r2_score(sample_train[f], predTrain)\n",
    "    r2test =r2_score(sample_test[f], predVal)\n",
    "    print('r2 scores', r2train,r2test,f)\n",
    "    r2dict[f] = {'train':r2train, 'test':r2test}\n",
    "    gbm.save_model('individualLGBMs_sampled/model_'+f+'_'+str(round(r2test,3))+'.txt')\n",
    "    gbm.save_model('individualLGBMs_sampled/model_'+f+'.txt')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ptend_t_0\n",
    "#base: r2 scores -3.594 ptend_t_0\n",
    "#addF: r2 scores 0.9354909909280742 -5.574277330887769 ptend_t_0\n",
    "#with callback: r2 scores 0.17447142147913053 0.10560585670117484 ptend_t_0 ->doesn't fit well at all\n",
    "\n",
    "#ptend_q0002_26\n",
    "#with callback: r2 scores 0.999897657944176 -7095520.075311637 ptend_q0002_26 -> doesn't fit well, we overpredict too large values\n",
    "#without weight: r2 scores 0.8236448689419253 -0.5606788450609883 ptend_q0002_26\n",
    "#without weight, 200 rounds: r2 scores 0.9269528462506769 -102.56223446349581 ptend_q0002_26\n",
    "plt.scatter(x=range(sample_test.shape[0]),y=sample_test[f], s=1,label=f)\n",
    "plt.scatter(x=range(sample_test.shape[0]),y=predVal, s=1,label='pred_test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=range(sample_train.shape[0]),y=sample_train[f], s=1,label=f)\n",
    "plt.scatter(x=range(sample_train.shape[0]),y=predTrain, s=1,label='pred_train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('leap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d127f3b59cdcbede3105ef79393826088284249e767f609c5a6124df566c40a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
