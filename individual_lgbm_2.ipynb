{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "import dask_ml\n",
    "import dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Sending large graph.*\")\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.multiprocessing\n",
    "\n",
    "cluster = LocalCluster(processes=True,n_workers=6, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "import sys\n",
    "import pickle \n",
    "\n",
    "from data_helpers import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    #'train0_25',\n",
    "    #'train25_50',\n",
    "    'train50_75',\n",
    "    #'train75_100'\n",
    "]\n",
    "\n",
    "# Read Parquet files from each folder into Dask DataFrames\n",
    "dfs = [dd.read_parquet(folder) for folder in folders]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "data = dd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "orig_partitions = [i for i in range(0,int(data.npartitions))]\n",
    "np.random.shuffle(orig_partitions) #shuffles inplace\n",
    "\n",
    "trainSep = int(0.95* data.npartitions)\n",
    "valEnd = data.npartitions #int(0.05* data.npartitions) + trainSep\n",
    "\n",
    "sampledPartIdxTrain = orig_partitions[0:trainSep]\n",
    "sampledPartIdxTest  = orig_partitions[trainSep:valEnd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find min values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfTarg =['ptend_q0002']# ['ptend_q0001','ptend_q0002','ptend_q0003']\n",
    "transfTarg60 = []\n",
    "for f in transfTarg:\n",
    "    for i in range(60):\n",
    "        transfTarg60.append(f+'_'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minDict = {} #minimum value that is not 0\n",
    "for f in ['ptend_q0001_26','ptend_q0002_26']:#transfTarg60: #allT:\n",
    "    a = data[f].compute()\n",
    "    hasPos = max(a)>0\n",
    "    hasNeg = min(a)<0\n",
    "    minNeg = min(abs(a.loc[a < 0])) if hasNeg else 1e10\n",
    "    minPos = min(abs(a.loc[a > 0])) if hasPos else 1e10\n",
    "    maxPos = max(a)\n",
    "    maxNeg = abs(min(a))\n",
    "    minDict[f] = {'minNeg':minNeg, 'minPos':minPos, 'min':min(minNeg,minPos), 'maxPos':maxPos, 'maxNeg':maxNeg, 'max':max(maxPos, maxNeg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('minVal_ptend_q0002_26.pkl', 'wb') as f:\n",
    "    pickle.dump(minDict, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('minVal_ptend_q0002_26.pkl', 'rb') as f:\n",
    "    minDict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try just with one dataset of 50 part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTensorDataFlattendPredictNextTimeStamp(data, partPerLoop, startPartIdx,sampledPartIdx):\n",
    "    dfList = []\n",
    "    for j in range(partPerLoop):\n",
    "        a = data.get_partition(int(sampledPartIdx[startPartIdx+j])).compute()\n",
    "        a, newF = addFeatures(a)\n",
    "\n",
    "        # transform targets\n",
    "        transfTarg = ['ptend_q0001','ptend_q0002','ptend_q0003']\n",
    "        transfF0 = ['state_q0001','state_q0002','state_q0003']\n",
    "        transfTargList = []\n",
    "        colDict={}\n",
    "        for ind,f in enumerate(transfTarg):\n",
    "            for i in range(60):\n",
    "                transfF = f+'_'+str(i)+'_transf'\n",
    "                colDict[transfF] = a[transfF0[ind]+'_'+str(i)]+a[f+'_'+str(i)]*1200\n",
    "                transfTargList.append(transfF)\n",
    "        a = pd.concat([a, pd.DataFrame(colDict)], axis=1)\n",
    "\n",
    "        allF = features60+newF+feat1\n",
    "        dfList.append(a)\n",
    "    \n",
    "    return pd.concat(dfList), allF, transfTargList\n",
    "\n",
    "def custom_x_inv(x):\n",
    "    return np.nan_to_num(1/(100*x), nan=0.0)\n",
    "\n",
    "def custom_log(x, minValue, offset=6):  #offset of works for [-403:403] of x values otherwise sign is lost\n",
    "    modMin = -minValue #* 0.9\n",
    "    x[x==0] = modMin # will make problems bc 0 could be positive but also negative! dynamics will point in different directions\n",
    "    y = np.log(abs(x))\n",
    "    #y[x==0] = -1e50  #replace infinities with 0 -> problem, can't learn that after very small x = large y, there should be 0 -> need a different mapping\n",
    "    y = y - offset           #move curve down such that we have a bigger domain that always has negative values as an outcome [-403:403]\n",
    "    y = np.sign(x)*abs(y)    #return sign information\n",
    "\n",
    "    y = y + abs(np.log((abs(modMin))))\n",
    "    return y\n",
    "\n",
    "def inv_custom_log(y,minValue, offset=6):\n",
    "    modMin = -minValue #* 0.9\n",
    "    y = y - abs(np.log((abs(modMin))))\n",
    "    x = np.exp(-abs(y) + offset)\n",
    "    #x[y == 1e-100] = 0       # not needed since\n",
    "    x = np.sign(y)*x\n",
    "    x[x== modMin] = 0\n",
    "    return x\n",
    "\n",
    "def getTensorDataFlattendPredictLog(data, partPerLoop, startPartIdx,sampledPartIdx):\n",
    "    dfList = []\n",
    "    for j in range(partPerLoop):\n",
    "        a = data.get_partition(int(sampledPartIdx[startPartIdx+j])).compute()\n",
    "        a, newF = addFeatures(a)\n",
    "\n",
    "        # transform targets\n",
    "        transfTarg = ['ptend_q0001','ptend_q0002']#['ptend_q0001','ptend_q0002','ptend_q0003']\n",
    "        transfTargList = []\n",
    "        colDict={}\n",
    "        for ind,f in enumerate(transfTarg):\n",
    "            for i in [26]: #range(60):\n",
    "                feature = f+'_'+str(i)\n",
    "                transfF = feature+'_transf'\n",
    "                minValue = minDict[feature]['min']\n",
    "                colDict[transfF] = custom_log(a[feature].copy(), minValue=minValue)\n",
    "                transfTargList.append(transfF)\n",
    "        a = pd.concat([a, pd.DataFrame(colDict)], axis=1)\n",
    "\n",
    "        allF = features60+newF+feat1\n",
    "        dfList.append(a)\n",
    "    \n",
    "    return pd.concat(dfList), allF, transfTargList\n",
    "\n",
    "def concatData(data, partPerLoop, startPartIdx,sampledPartIdx):\n",
    "    dfList = []\n",
    "    for j in range(partPerLoop):\n",
    "        a = data.get_partition(int(sampledPartIdx[startPartIdx+j])).compute()\n",
    "        a, newF = addFeatures(a)\n",
    "\n",
    "        allF = features60+newF+feat1\n",
    "        dfList.append(a)\n",
    "    \n",
    "    return pd.concat(dfList), allF, allF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data\n",
    "partPerLoop = 35\n",
    "\n",
    "for i in range(1):\n",
    "    startPartIdx = i*partPerLoop\n",
    "    val, combinedF,transT = concatData(data, partPerLoop, startPartIdx, sampledPartIdxTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training sequentially\n",
    "partPerLoop = 25\n",
    "\n",
    "for i in range(1):\n",
    "    startPartIdx = i*partPerLoop\n",
    "    train, combinedF,transT = concatData(data, partPerLoop, startPartIdx, sampledPartIdxTrain)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find mapping for tiny values\n",
    "\n",
    "ATTENTION: will not improve r2 score -> most of the error of r2 is caused by outliers / values far from mean need to be predicted well for a good r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logarithm mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train.ptend_q0002_26\n",
    "#a[a==0]= 0.9*minDict['ptend_q0002_26']['min']\n",
    "plt.hist(np.log(a), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=range(0,train.shape[0]), y=np.log(a), s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=range(0,train.shape[0]), y=train.ptend_q0002_26, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=range(0,train.shape[0]), y=train.ptend_q0002_26, s=1)\n",
    "plt.scatter(x=range(0,train.shape[0]), y=inv_custom_log(train.ptend_q0002_26_transf,minDict['ptend_q0002_26']['min']), s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=range(0,train.shape[0]), y=train.ptend_q0002_26_transf, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_info = np.finfo(train[f].dtype.type)\n",
    "\n",
    "precision = precision_info.precision\n",
    "precision,precision_info.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(np.log(1e-50)), 1e-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# attention you loose the sign with this one!!!!\n",
    "def sech_squared(x,amplitude=10e4,freq=500):\n",
    "    return np.sign(x)* amplitude / np.cosh(freq*x)\n",
    "def inverse_sech_squared(y, amplitude=10e4, freq=50):\n",
    "    return (1 / freq) * np.arccosh(np.sqrt(amplitude / np.abs(y))) * np.sign(y)\n",
    "\n",
    "def custom_log(x, minValue, offset=6):  #offset of works for [-403:403] of x values otherwise sign is lost\n",
    "    x[x==0] = minValue * 0.5 # will make problems bc 0 could be positive but also negative! dynamics will point in different directions\n",
    "    y = np.log(abs(x))\n",
    "    #y[x==0] = -1e50  #replace infinities with 0 -> problem, can't learn that after very small x = large y, there should be 0 -> need a different mapping\n",
    "    y = y - offset           #move curve down such that we have a bigger domain that always has negative values as an outcome [-403:403]\n",
    "    y = np.sign(x)*abs(y)    #return sign information\n",
    "    return y\n",
    "\n",
    "def inv_custom_log(y,offset=6):\n",
    "    x = np.exp(-abs(y) + offset)\n",
    "    #x[y == 1e-100] = 0       # not needed since\n",
    "    x = np.sign(y)*x\n",
    "    return x\n",
    "\n",
    "x0 = 0\n",
    "L = 10\n",
    "k = 200\n",
    "x = np.linspace(-2,2,21)\n",
    "sigmoid = L / (1 + np.exp(-k*(x - x0)))\n",
    "y = custom_log(x)\n",
    "inv = inv_custom_log(y)\n",
    "fig = plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,inv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cox-box transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = st.boxcox(train['ptend_q0002_26'] + abs(train['ptend_q0002_26']), lmbda= 0.5)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(x=range(0,train.shape[0]), y=trans, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quantile transfomer from sklearn\n",
    "q0001:\n",
    "- uniform dist:  r2 scores -0.0024028623201599597 -0.14726808308227235 transormed 0.5795544569557314 0.46740129096623895\n",
    "- normal:        r2 scores -6.038224021203636 -5.831603829623485 transormed 0.7500584769750813 0.6309097700251043\n",
    "\n",
    "q0002:\n",
    "- uniform: r2 scores 9.495082898713925e-06 -9.13032750657905e-05 transormed 0.9597399802188882 0.9419017458308543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QuantileTransformer(n_quantiles=10000, random_state=0, output_distribution='uniform')\n",
    "quantileT = qt.fit_transform(train[['ptend_q0002_26']])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(x=range(0,train.shape[0]), y=quantileT, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invT = qt.inverse_transform(quantileT)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(x=range(0,train.shape[0]), y=invT, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = 'ptend_q0002_26'\n",
    "f = 'ptend_q0002_26_transf'\n",
    "\n",
    "#f0 = 'ptend_u_42'\n",
    "#f = f0+'_transf'\n",
    "#\n",
    "#f0 = 'ptend_q0003_15'\n",
    "#f = f0+'_transf'\n",
    "\n",
    "qt = QuantileTransformer(n_quantiles=10000, random_state=0,output_distribution='uniform')\n",
    "train[f] = qt.fit_transform(train[[f0]])\n",
    "val[f]   = qt.transform(val[[f0]])\n",
    "\n",
    "#valSet = lgb.Dataset(val[combinedF], label=val[f], free_raw_data=False)\n",
    "#train_set = lgb.Dataset(train[combinedF], train[f], free_raw_data=False)\n",
    "\n",
    "valSet = lgb.Dataset(val.loc[val[f0] != 0][combinedF], label=val.loc[val[f0] != 0][f], free_raw_data=False)\n",
    "train_set = lgb.Dataset(train[train[f0] != 0][combinedF], train.loc[train[f0] != 0][f], free_raw_data=False)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    #'num_leaves': 15,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print('processing ',f)\n",
    "fileName = 'individualLGBMs_feat/model_'+f+'.txt'\n",
    "gbm = None #lgb.Booster(model_file=fileName) if i != 0 else None\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "            train_set,\n",
    "            num_boost_round=500, \n",
    "            valid_sets=valSet,\n",
    "            #callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=True)],\n",
    "            init_model=gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTrain0 = gbm.predict(train[train[f0] != 0][combinedF])\n",
    "predVal0 = gbm.predict(val[val[f0] != 0][combinedF])\n",
    "predTrain = qt.inverse_transform(np.reshape(predTrain0,(-1,1)))\n",
    "predVal = qt.inverse_transform(np.reshape(predVal0,(-1,1)))\n",
    "r2train =r2_score(np.reshape(train[train[f0] != 0][f0], (-1,1)), predTrain)\n",
    "r2test =r2_score(np.reshape(val[val[f0] != 0][f0], (-1,1)), predVal)\n",
    "print('r2 scores', r2train,r2test, 'transormed',r2_score(train[train[f0] != 0][f], predTrain0),r2_score(val[val[f0] != 0][f], predVal0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train[train[f0] != 0][f0]\n",
    "((a-a.mean())**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a- a.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ((np.reshape(a, (-1,1)) - predTrain)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (np.reshape(a, (-1,1)) - predTrain)**2 / mse *100\n",
    "plt.scatter(x=range(predTrain0.shape[0]),y=b, s=1,label='gt')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train[f0] != 0][f0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quantile transformer self implemented\n",
    "\n",
    "choose closest neighbor\n",
    "- q0002: r2 scores 0.00031506126944913504 -0.027846289869214003 transormed 0.9940111528867572 0.9916804881993494\n",
    "- q0001: r2 scores -0.2750321239707938 -0.6288146334566054 transormed 0.5818159792591474 0.4669909993569028\n",
    "\n",
    "\n",
    "interpolation:\n",
    "- q0002 r2 scores 0.0004193936653468233 8.448629331803126e-06 transormed 0.9940111528867572 -0.4430557552761931\n",
    "- q0001 r2 scores -5.252347836889369 -4.9835035690989375 transormed 0.5818159792591474 -3.423471356255156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Attention: the origValues and the quantiles have to have the same indexing, e.g. origValues[1] = f[quantiles[1]], means this represents the mapping between quantile and orig value\n",
    "\"\"\"\n",
    "def reverseMapping(predictQuantiles, origValues, quantiles):\n",
    "    results = np.ones((predictQuantiles.shape[0],1)) * np.nan\n",
    "    for i,val in enumerate(predictQuantiles):\n",
    "        idx = np.argmin(abs(quantiles - val))\n",
    "        results[i] = origValues[idx]\n",
    "    return results\n",
    "\n",
    "def mapping(values, origValues, quantiles):\n",
    "    results = np.ones((values.shape[0],1)) * np.nan\n",
    "    for i,val in enumerate(values):\n",
    "        idx = np.argmin(abs(origValues - val))\n",
    "        results[i] = quantiles[idx]\n",
    "    return results\n",
    "\n",
    "\n",
    "# get original values for predicted quantiles\n",
    "def reverseMappingInterp(predictQuantiles, origValues, quantiles):\n",
    "    results = np.ones((predictQuantiles.shape[0],1)) * np.nan\n",
    "\n",
    "    for i,val in enumerate(predictQuantiles):\n",
    "        idx_lower = np.argmax(quantiles <= val)\n",
    "        idx_upper = np.argmin(quantiles >= val)\n",
    "    \n",
    "        if idx_lower == idx_upper:\n",
    "            results[i] = origValues[idx_upper]\n",
    "        else:\n",
    "            # Linear interpolation\n",
    "            fraction = (val - quantiles[idx_lower]) / (quantiles[idx_upper] - quantiles[idx_lower])\n",
    "            results[i] = origValues[idx_lower] + (origValues[idx_upper] - origValues[idx_lower]) * fraction\n",
    "    return results\n",
    "\n",
    "# get quantiles from some original Values\n",
    "def mappingInterp(values, origValues, quantiles):\n",
    "    results = np.ones((values.shape[0],1)) * np.nan\n",
    "\n",
    "    for i,val in enumerate(values):\n",
    "        idx_lower = np.argmax(origValues <= val)\n",
    "        idx_upper = np.argmin(origValues >= val)\n",
    "    \n",
    "        if idx_lower == idx_upper:\n",
    "            results[i] = quantiles[idx_upper]\n",
    "        else:\n",
    "            fraction = (val - origValues[idx_lower]) / (origValues[idx_upper] - origValues[idx_lower])\n",
    "            results[i] = quantiles[idx_lower] + (quantiles[idx_upper] - quantiles[idx_lower]) * fraction\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['rank'] = train['ptend_q0002_26'].rank(method='dense', ascending=True)\n",
    "train['quantiles'] = train['rank'] / train.shape[0]\n",
    "\n",
    "predicts = train.quantiles.to_numpy()\n",
    "ranks = train['rank'].to_numpy()\n",
    "quantiles = train.quantiles.to_numpy()\n",
    "origValues = train['ptend_q0002_26'].to_numpy()\n",
    "res = reverseMapping(predicts, ranks,quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = mapping(val['ptend_q0002_26'].to_numpy(), origValues,quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = 'ptend_q0002_26'\n",
    "f = 'ptend_q0002_26_transf'\n",
    "\n",
    "train['rank'] = train[f0].rank(method='dense', ascending=True)\n",
    "train['quantiles'] = train['rank'] / train.shape[0]\n",
    "\n",
    "quantiles = train.quantiles.to_numpy()\n",
    "origValues = train[f0].to_numpy()\n",
    "\n",
    "train[f] = train['quantiles']\n",
    "val[f] = mappingInterp(val[f0].to_numpy(), origValues,quantiles)\n",
    "\n",
    "valSet = lgb.Dataset(val[combinedF], label=val[f], free_raw_data=False)\n",
    "train_set = lgb.Dataset(train[combinedF], train[f], free_raw_data=False)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    #'num_leaves': 15,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print('processing ',f)\n",
    "fileName = 'individualLGBMs_feat/model_'+f+'.txt'\n",
    "gbm = None #lgb.Booster(model_file=fileName) if i != 0 else None\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "            train_set,\n",
    "            num_boost_round=100, \n",
    "            valid_sets=valSet,\n",
    "            init_model=gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTrain0 = gbm.predict(train[combinedF])\n",
    "predVal0 = gbm.predict(val[combinedF])\n",
    "predTrain = reverseMappingInterp(np.reshape(predTrain0,(-1,1)), origValues,quantiles)\n",
    "predVal = reverseMappingInterp(np.reshape(predVal0,(-1,1)), origValues,quantiles)\n",
    "r2train =r2_score(train[f0], predTrain)\n",
    "r2test =r2_score(val[f0], predVal)\n",
    "print('r2 scores', r2train,r2test, 'transormed',r2_score(train[f], predTrain0),r2_score(val[f], predVal0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(x=range(quantiles.shape[0]),y=quantiles, s=1,label='quantiles')\n",
    "#plt.scatter(x=range(values.shape[0]),y=values, s=1,label='values')\n",
    "plt.scatter(x=range(train['ptend_q0002_26_transf'].shape[0]),y=train['ptend_q0002_26_transf'], s=1,label='x_transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### folded powers transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log transform testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = 'ptend_q0001_26'\n",
    "f = 'ptend_q0001_26_transf'\n",
    "\n",
    "valSet = lgb.Dataset(val[combinedF], label=val[f], free_raw_data=False)\n",
    "train_set = lgb.Dataset(train[combinedF], train[f], free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = 'ptend_q0002_26'\n",
    "f = 'ptend_q0002_26_transf'\n",
    "\n",
    "subV = val.loc[val[f0] != 0]\n",
    "subT = train.loc[train[f0] != 0]\n",
    "\n",
    "valSet = lgb.Dataset(subV[combinedF], label=subV[f], free_raw_data=False)\n",
    "train_set = lgb.Dataset(subT[combinedF], subT[f], free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    #'num_leaves': 15,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "\n",
    "print('processing ',f)\n",
    "fileName = 'individualLGBMs_feat/model_'+f+'.txt'\n",
    "gbm = None #lgb.Booster(model_file=fileName) if i != 0 else None\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "            train_set,\n",
    "            num_boost_round=100, \n",
    "            valid_sets=valSet,\n",
    "            init_model=gbm)\n",
    "\n",
    "predTrain0 = gbm.predict(subT[combinedF])\n",
    "predVal0 = gbm.predict(subV[combinedF])\n",
    "predTrain = inv_custom_log(predTrain0, minDict[f0]['min'])\n",
    "predVal = inv_custom_log(predVal0, minDict[f0]['min'])\n",
    "r2train =r2_score(subT[f0], predTrain)\n",
    "r2test =r2_score(subV[f0], predVal)\n",
    "#r2ScoreDict[f][i] = {'train':r2train,'test':r2test}\n",
    "print('r2 scores', r2train,r2test, 'transormed',r2_score(subT[f], predTrain0),r2_score(subV[f], predVal0))\n",
    "#gbm.save_model(fileName)\n",
    "#gbm.save_model('individualLGBMs_feat/checkpoints/model_'+f+'_'+str(i)+'_'+str(round(r2test,3))+'.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('r2 scores', r2train,r2test, 'transormed',r2_score(subT[f], predTrain0),r2_score(subV[f], predVal0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r2 scores 0.0002870120951724564 -2.0425050317740556e-05    q0002\n",
    "#r2 scores -7.53548410945837e+19 -7.067963969118257e+19     q0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: need to predict magnitude and sign separate: by using the log to map the network is able to predict very small changes,\n",
    "#       but probably will have issues with 0 since they all will have one sign (-) & then the dynamics don't fit together, \n",
    "#       a small change in state space = big change in output (- to +) -200 -> + 200\n",
    "# -> test and evaluate this!!\n",
    "\n",
    "# -> target needs a continuous form, gbm needs some values between 0 and not 0\n",
    "\n",
    "# -> even when predicting targets != 0 the r value is not good, maybe because the small changes in the system are highly relevant -> need a different mapping\n",
    "#       in transofrmed domain we fit very well! just not when we transform back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.best_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization in transformed space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=range(predTrain0.shape[0]),y=train[train[f0] != 0][f], s=1,label='gt',alpha=0.5)\n",
    "plt.scatter(x=range(predTrain0.shape[0]),y=predTrain0, s=1,label='pred',alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=range(predVal0.shape[0]),y=predVal0, s=1,label='pred')\n",
    "plt.scatter(x=range(predVal0.shape[0]),y=val[val[f0] != 0][f], s=1,label='gt')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization in plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "# Data preparation\n",
    "x = list(range(predTrain0.shape[0]))\n",
    "y_gt = train[train[f0] != 0][f].values\n",
    "y_pred = predTrain0\n",
    "\n",
    "trace_gt = go.Scatter(x=x,y=y_gt,mode='markers',marker=dict(size=4, color='green'),name='gt')\n",
    "trace_pred = go.Scatter(x=x,y=y_pred,mode='markers',marker=dict(size=2,color='red'),name='pred')\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces to the figure\n",
    "fig.add_trace(trace_gt)\n",
    "fig.add_trace(trace_pred)\n",
    "\n",
    "# Add legend\n",
    "fig.update_layout(legend=dict(title=\"Legend\"),showlegend=True)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(predTrain.shape[0]))\n",
    "y_gt = train[train[f0] != 0][f0].values\n",
    "y_pred = predTrain\n",
    "\n",
    "trace_gt = go.Scatter(x=x,y=y_gt,mode='markers',marker=dict(size=4, color='green'),name='gt')\n",
    "trace_pred = go.Scatter(x=x,y=y_pred,mode='markers',marker=dict(size=2,color='red'),name='pred')\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces to the figure\n",
    "fig.add_trace(trace_gt)\n",
    "fig.add_trace(trace_pred)\n",
    "\n",
    "# Add legend\n",
    "fig.update_layout(legend=dict(title=\"Legend\"),showlegend=True)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization in target space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=range(predTrain.shape[0]),y=predTrain, s=1,label='pred')\n",
    "plt.scatter(x=range(predTrain.shape[0]),y=train[f0], s=1,label='gt')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=range(predTrain.shape[0]),y=predTrain, s=1,label='pred')\n",
    "plt.scatter(x=range(predTrain.shape[0]),y=train[f0], s=1,label='gt')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTargets = targets60+target1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(y, axis=0)\n",
    "std = np.std(y, axis=0)\n",
    "std[std==0] = 1\n",
    "\n",
    "yn = (y - mean) / std\n",
    "yn_val = (y_val - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    'num_leaves': 15,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "\n",
    "r2ScoreDict = {f: {} for f in allTargets}\n",
    "i=0\n",
    "for idx,f in enumerate(allTargets):\n",
    "    print('processing ',f)\n",
    "    fileName = 'individualLGBMs_feat/model_'+f+'.txt'\n",
    "    gbm = lgb.Booster(model_file=fileName) if i != 0 else None\n",
    "\n",
    "    valSet = lgb.Dataset(X_val, label=yn_val[:,idx], free_raw_data=False)\n",
    "    train_set = lgb.Dataset(X, yn[:,idx], free_raw_data=False)\n",
    "    gbm = lgb.train(params,\n",
    "                train_set,\n",
    "                num_boost_round=20, \n",
    "                valid_sets=valSet,\n",
    "                init_model=gbm)\n",
    "    \n",
    "    predTrain = gbm.predict(X)\n",
    "    predVal = gbm.predict(X_val)\n",
    "\n",
    "    predTrain = predTrain*std[idx] + mean[idx]\n",
    "    predVal = predVal *std[idx] + mean[idx]\n",
    "    r2train =r2_score(y[:,idx], predTrain)\n",
    "    r2test =r2_score(y_val[:,idx], predVal)\n",
    "    r2ScoreDict[f][i] = {'train':r2train,'test':r2test}\n",
    "    print('r2 scores', r2train,r2test)\n",
    "    gbm.save_model(fileName)\n",
    "    gbm.save_model('individualLGBMs_feat/checkpoints/model_'+f+'_'+str(i)+'_'+str(round(r2test,3))+'.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q0002_26 -> outlier removal, seems like there are some cases where it's off\n",
    "# regenerate plots, index needs to be resetted i guess\n",
    "# -> better: use exp(targ) to have a better distinguishable target\n",
    "\n",
    "# MULTIPLY by time! dt = 1200sec, maybe transform to abs value, instead of predicting flux, predict abs value\n",
    "#e.g. T1 = t0+flux -> flux = (t1-t0)*1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaled baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'ptend_q0002_26'\n",
    "\n",
    "valSet = lgb.Dataset(val[combinedF], label=val[f]/minDict[f]['min'], free_raw_data=False)\n",
    "trainSet = lgb.Dataset(train[combinedF], train[f]/minDict[f]['min'], free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minDict[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(valSet.label,(-1,1)) * minDict[f]['min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=range(val.shape[0]),y=valSet.label*minDict[f]['min'], s=1,label=f)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    #'num_leaves': 15,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "            train_set,\n",
    "            num_boost_round=100, \n",
    "            valid_sets=valSet,\n",
    "            init_model=None)\n",
    "\n",
    "predTrain0 = gbm.predict(trainSet.data)\n",
    "predVal0 = gbm.predict(valSet.data)\n",
    "predTrain = predTrain0*minDict[f]['min']\n",
    "predVal = predVal0 *minDict[f]['min']\n",
    "r2train =r2_score(trainSet.label*minDict[f]['min'], predTrain)\n",
    "r2test =r2_score(valSet.label*minDict[f]['min'], predVal)\n",
    "print('r2 scores', r2train,r2test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try with multiple part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('leap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d127f3b59cdcbede3105ef79393826088284249e767f609c5a6124df566c40a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
