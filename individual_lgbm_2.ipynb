{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "import dask_ml\n",
    "import dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Sending large graph.*\")\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.multiprocessing\n",
    "\n",
    "cluster = LocalCluster(processes=True,n_workers=6, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "import sys\n",
    "from data_helpers import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    'train0_25',\n",
    "    'train25_50',\n",
    "    'train50_75',\n",
    "    'train75_100'\n",
    "]\n",
    "\n",
    "# Read Parquet files from each folder into Dask DataFrames\n",
    "dfs = [dd.read_parquet(folder) for folder in folders]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "data = dd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "orig_partitions = [i for i in range(0,int(data.npartitions))]\n",
    "np.random.shuffle(orig_partitions) #shuffles inplace\n",
    "\n",
    "trainSep = int(0.95* data.npartitions)\n",
    "valEnd = data.npartitions #int(0.05* data.npartitions) + trainSep\n",
    "\n",
    "sampledPartIdxTrain = orig_partitions[0:trainSep]\n",
    "sampledPartIdxTest  = orig_partitions[trainSep:valEnd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try just with one dataset of 50 part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data\n",
    "partPerLoop = 35\n",
    "\n",
    "for i in range(1):\n",
    "    startPartIdx = i*partPerLoop\n",
    "    X_val, y_val, combinedF,_ = getTensorDataFlattend(data, partPerLoop, startPartIdx, sampledPartIdxTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training sequentially\n",
    "partPerLoop = 50\n",
    "\n",
    "for i in range(1):\n",
    "    startPartIdx = i*partPerLoop\n",
    "    X,y, combinedF,_ = getTensorDataFlattend(data, partPerLoop, startPartIdx, sampledPartIdxTrain)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTargets = targets60+target1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(y, axis=0)\n",
    "std = np.std(y, axis=0)\n",
    "std[std==0] = 1\n",
    "\n",
    "yn = (y - mean) / std\n",
    "yn_val = (y_val - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    'num_leaves': 15,\n",
    "    #'learning_rate': 0.05,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "\n",
    "r2ScoreDict = {f: {} for f in allTargets}\n",
    "i=0\n",
    "for idx,f in enumerate(allTargets):\n",
    "    print('processing ',f)\n",
    "    fileName = 'individualLGBMs_feat/model_'+f+'.txt'\n",
    "    gbm = lgb.Booster(model_file=fileName) if i != 0 else None\n",
    "\n",
    "    valSet = lgb.Dataset(X_val, label=yn_val[:,idx], free_raw_data=False)\n",
    "    train_set = lgb.Dataset(X, yn[:,idx], free_raw_data=False)\n",
    "    gbm = lgb.train(params,\n",
    "                train_set,\n",
    "                num_boost_round=20, \n",
    "                valid_sets=valSet,\n",
    "                init_model=gbm)\n",
    "    \n",
    "    predTrain = gbm.predict(X)\n",
    "    predVal = gbm.predict(X_val)\n",
    "\n",
    "    predTrain = predTrain*std[idx] + mean[idx]\n",
    "    predVal = predVal *std[idx] + mean[idx]\n",
    "    r2train =r2_score(y[:,idx], predTrain)\n",
    "    r2test =r2_score(y_val[:,idx], predVal)\n",
    "    r2ScoreDict[f][i] = {'train':r2train,'test':r2test}\n",
    "    print('r2 scores', r2train,r2test)\n",
    "    gbm.save_model(fileName)\n",
    "    gbm.save_model('individualLGBMs_feat/checkpoints/model_'+f+'_'+str(i)+'_'+str(round(r2test,3))+'.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q0002_26 -> outlier removal, seems like there are some cases where it's off\n",
    "# regenerate plots, index needs to be resetted i guess\n",
    "# -> better: use exp(targ) to have a better distinguishable target\n",
    "\n",
    "# MULTIPLY by time! dt = 1200sec, maybe transform to abs value, instead of predicting flux, predict abs value\n",
    "#e.g. T1 = t0+flux -> flux = (t1-t0)*1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try with multiple part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('leap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d127f3b59cdcbede3105ef79393826088284249e767f609c5a6124df566c40a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
